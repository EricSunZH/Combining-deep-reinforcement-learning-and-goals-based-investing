% Encoding: UTF-8

@Article{Amenc-et-al-2009,
  author               = {Amenc, Noel and Martellini, Lionel and Milhau, Vincent and Ziemann, Volker},
  date                 = {2009-10},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Asset-Liability Management in Private Wealth Management},
  doi                  = {10.3905/jpm.2009.36.1.100},
  issn                 = {0095-4918},
  number               = {1},
  pages                = {100--120},
  volume               = {36},
  abstract             = {The objective of this article is to shed light on the potential benefits of asset-liability management techniques, originally developed for institutional money management, in a private wealth management context. The authors show that much of the complexity of optimal asset allocation decisions for private investors can be captured through the addition of a single state variable liability value which accounts in a parsimonious way for investors' specific constraints and objectives. An asset-liability management approach to private wealth management has a direct impact on the selection of asset classes because it requires a consideration of the liability-hedging properties of various asset classes, that would, by definition, be absent from an asset-only perspective. An asset-liability perspective also leads to the use of the liability portfolio as a benchmark, or numeraire, acknowledging that, for private investors, terminal wealth per se is not as important as the investor's ability to achieve goals, such as preparing for retirement or buying property.},
  citeulike-article-id = {13990222},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2009.36.1.100},
  groups               = {Goals based investing, Goals_Invest},
  journal              = {The Journal of Portfolio Management},
  owner                = {cristi},
  posted-at            = {2016-03-29 11:19:11},
  timestamp            = {2020-02-28 03:32},
  year                 = {2009},
}

@Article{Baker-Ricciardi-2015,
  author               = {Baker, H. Kent and Ricciardi, Victor},
  date                 = {2015-04},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Understanding Behavioral Aspects of Financial Planning and Investing},
  url                  = {https://ssrn.com/abstract=2596202},
  abstract             = {Understanding fundamental human tendencies can help financial planners and advisers recognize behaviors that may interfere with clients achieving their long-term goals. The authors describe several well-established behavioral biases and suggest how to overcome them.},
  citeulike-article-id = {13707017},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2596202},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2596202code262799.pdf?abstractid=2596202 and mirid=1},
  day                  = {20},
  groups               = {Goals_Invest},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2596202},
  owner                = {zkgst0c},
  posted-at            = {2016-02-06 05:20:47},
  timestamp            = {2020-02-28 03:32},
}

@Article{Blanchett-2015a,
  author               = {Blanchett, David},
  date                 = {2015},
  journaltitle         = {Journal of Financial Planning},
  title                = {The Value of Goals-Based Financial Planning},
  url                  = {https://www.onefpa.org/journal/Pages/JUN15-The-Value-of-Goals-Based-Financial-Planning.aspx},
  abstract             = {The financial planning profession is built on helping people accomplish goals. While investing appropriately is generally an important part of accomplishing a goal, achieving a goal often requires advice beyond selecting investments based on alpha and beta.

Past research on the topic of goals-based financial planning has focused primarily on determining optimal portfolios to fund different types of goals. In contrast, the focus of this research is how to determine which goals should be funded, as well as how to optimally save toward goals over time. A utility model based on prospect theory was used to determine the optimal funding strategy for a household.

The results suggest that using a goals-based framework to determine which goals to fund and how to fund them can lead to an increase in utility-adjusted wealth of 15.09 percent for a hypothetical household versus a na ive strategy focused only on funding retirement. This is equivalent to generating an annual alpha of 1.65 percent for the lifetime of the base scenario household.},
  citeulike-article-id = {13930928},
  groups               = {Goals based investing, Goals_Invest, Individ_Goals, Value_FinAdvice},
  howpublished         = {Available at https://www.onefpa.org/journal/pages/jun15-the-value-of-goals-based-financial-planning.aspx},
  owner                = {zkgst0c},
  posted-at            = {2016-02-10 22:29:36},
  timestamp            = {2020-02-28 03:32},
}

@Article{Bodie-2003,
  author               = {Bodie, Zvi},
  date                 = {2003-01},
  journaltitle         = {Financial Analysts Journal},
  title                = {Thoughts on the Future: Life-cycle Investing in Theory and Practice},
  doi                  = {10.2469/faj.v59.n1.2500},
  volume               = {59},
  abstract             = {Global population aging and deregulation have created opportunities for innovative investment firms to create new products and services that address the needs of people saving for retirement and other life-cycle goals. It will be a challenge to frame risk-reward trade-offs and cast financial decision making in a format that ordinary people can understand and implement. Fortunately, advances in financial science have made possible a new generation of user-friendly investment products.

Among the important insights of modern financial science are the following:

A person's welfare depends not only on her end-of-period wealth but also on the consumption of goods and leisure over her entire lifetime.

Multiperiod hedging (rather than time diversification) is the way to manage market risk over time.

Portfolio managers can and should make greater use of the information about interest rates and implied volatilities embedded in the prices of derivatives, such as swaps and options.

The value, riskiness, and flexibility of a person's labor earnings are of first-order importance in optimal portfolio selection at each stage of the life cycle.

Habit formation can give rise to a demand for guarantees against a decline in investment income.

Because of transaction costs, agency problems, and limited knowledge on the part of consumers, dynamic asset allocation will and should become an activity performed by financial intermediaries, rather than by their retail customers.

The tendency in the last several years has been to offer participants in self-directed retirement plans more and more investment alternatives. But when people do not have the knowledge to make choices in their own best interests, increasing the number of alternative investments does not necessarily make them better off. In fact, it may make them more vulnerable to exploitation by opportunistic salespeople or by well-intentioned but unqualified professionals.

The modern theory of contingent-claims analysis provides the framework for the production and pricing of new and improved life-cycle contracts that combine features of insurance and investment. For example, consider an escalating life annuity with a minimum benefit linked to the cost of living. Payments would increase with inflation and with the performance of a market index, and increases would be locked in for life. It could be dynamically hedged and priced by using default-free inflation-protected bonds and index futures or options. This product would be very different from a mutual-funds-based variable annuity contract, which passes the investment risks directly through to the consumer.

Longer life expectancies have coincided with increased health care costs near the end of peoples' lives, which raises the specter that many elderly people may need several years of expensive long-term care. A bundled insurance/investment contract could help to deal with this problem by combining an escalating life annuity with long-term care insurance. Combining the coverage would mitigate the adverse selection that can occur in the demand for each of the two products on a stand-alone basis.},
  citeulike-article-id = {13909006},
  citeulike-linkout-0  = {http://dx.doi.org/10.2469/faj.v59.n1.2500},
  groups               = {Goals based investing, Goals_Invest, Goals_Inflation, Retire_HealthCare, Annuities, OBPI},
  owner                = {zkgst0c},
  posted-at            = {2016-01-14 20:39:02},
  timestamp            = {2020-02-28 03:32},
}

@Article{Brunel-2011,
  author               = {Brunel, Jean L. P.},
  date                 = {2011-10},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Goal-Based Wealth Management in Practice},
  doi                  = {10.3905/jwm.2011.14.3.017},
  issn                 = {1534-7524},
  number               = {3},
  pages                = {17--26},
  volume               = {14},
  abstract             = {Though goals-based wealth management is certainly not a radically new discipline, it has recently assumed a more important role within the private wealth management industry. Two discrete factors probably stand behind this development: a change in client perceptions of risk after the 2008 market melt-down and the publication of the seminal piece by Das, Markowitz, Scheid, and Statman.

This article starts with a review of the key issues which families typically face when dealing with wealth planning, then discusses a framework which allows advisors to consider financial, estate, and investment-planning needs, shows the practical considerations associated with the process and concludes with a brief discussion of the major business challenges that still need to be addressed.},
  citeulike-article-id = {13968184},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2011.14.3.017},
  groups               = {Goals based investing, Goals_Invest, Individ_Goals},
  owner                = {cristi},
  posted-at            = {2016-03-05 20:51:28},
  timestamp            = {2020-02-28 03:32},
}

@Article{Cai-Ge-2012,
  author               = {Cai, Jun and Ge, Chenliang},
  date                 = {2012-05},
  journaltitle         = {Economic Modelling},
  title                = {Multi-objective private wealth allocation without subportfolios},
  doi                  = {10.1016/j.econmod.2011.11.013},
  issn                 = {0264-9993},
  number               = {3},
  pages                = {900--907},
  volume               = {29},
  abstract             = {As opposed to institutional investors, individual investors typically have several investment objectives in mind. The traditional utility maximization approach is not only oversimplified but also may not be suitable for real world application. Behavioral asset allocation divides a portfolio into subportfolios, which can cause potential problems. This paper follows the Modern Portfolio Theory and introduces the practical idea of treating some goals as constraints. How this works in practice is illustrated by an example of an individual having three different objectives. This article follows the idea of Chen et al. (2006) and includes life insurance. Consumption is modeled into three parts and accommodates a reasonable basis for calculating life insurance requirements and generally integrates consumption into the investment decision. As a whole, the model provides a customized solution for the environment and complex investment goals of an individual. We propose a novel approach to deal with the multiple objectives of an individual. We introduce the net human capital concept for calculating the extended portfolio. We analyze the relation between the optimal allocation and some important factors. Correct identification of objectives is most important for wealth allocation.},
  citeulike-article-id = {10354090},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.econmod.2011.11.013},
  groups               = {Goals_Invest, Human_Capital},
  owner                = {cristi},
  posted-at            = {2016-03-26 18:03:27},
  timestamp            = {2020-02-28 03:32},
}

@Article{Chhabra-2005,
  author               = {Chhabra, Ashvin B.},
  date                 = {2005-01},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Beyond Markowitz: A Comprehensive Wealth Allocation Framework for Individual Investors},
  doi                  = {10.3905/jwm.2005.470606},
  issn                 = {1520-4154},
  number               = {4},
  pages                = {8--34},
  volume               = {7},
  abstract             = {In sharp contrast to the recommendations of Modern Portfolio Theory, a vast majority of investors are not well diversified. The author attempts to provide a solution to this diversification paradox, by expanding the Markowitz framework of diversifying market risk to also include the concepts of personal risk and aspirational goals.

The wealth allocation framework enables individual investors to construct appropriate portfolios using all their assets, such as their home, mortgage, market investments, and human capital. The investor may choose to accept a slightly lower average rate of return in exchange for downside protection and upside potential.

The resulting portfolios are designed to meet individual investors' needs and preferences, as well as to protect individuals from personal, market, and aspirational risk factors. A major conclusion of this work is that, for the individual investor, risk allocation should precede asset allocation.},
  citeulike-article-id = {13937259},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2005.470606},
  groups               = {Goals based investing, Goals_Invest, Human_Capital},
  owner                = {zkgst0c},
  posted-at            = {2016-02-20 20:02:06},
  timestamp            = {2020-02-28 03:32},
}

@Book{Chhabra-2015,
  author               = {Chhabra, Ashvin B.},
  date                 = {2015-06},
  title                = {The Aspirational Investor: Taming the Markets to Achieve Your Life's Goals},
  publisher            = {HarperBusiness},
  url                  = {https://www.amazon.com/Aspirational-Investor-Taming-Markets-Achieve/dp/0062235095},
  citeulike-article-id = {13911140},
  citeulike-linkout-0  = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20 and amp;path=ASIN/B00NEOSFTM},
  citeulike-linkout-1  = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21 and amp;path=ASIN/B00NEOSFTM},
  citeulike-linkout-2  = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21 and amp;path=ASIN/B00NEOSFTM},
  citeulike-linkout-3  = {http://www.amazon.jp/exec/obidos/ASIN/B00NEOSFTM},
  citeulike-linkout-4  = {http://www.amazon.co.uk/exec/obidos/ASIN/B00NEOSFTM/citeulike00-21},
  citeulike-linkout-5  = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20 and path=ASIN/B00NEOSFTM},
  citeulike-linkout-6  = {http://www.worldcat.org/isbn/9780062235107},
  citeulike-linkout-7  = {http://books.google.com/books?vid=ISBN9780062235107},
  citeulike-linkout-8  = {http://www.amazon.com/gp/search?keywords=9780062235107 and index=books and linkCode=qs},
  citeulike-linkout-9  = {http://www.librarything.com/isbn/9780062235107},
  day                  = {02},
  groups               = {Goals based investing, Goals_Invest, Individ_Goals},
  howpublished         = {Kindle Edition},
  owner                = {cristi},
  posted-at            = {2016-01-17 18:52:13},
  timestamp            = {2020-02-28 03:32},
}

@Article{Arulkumaran-et-al-2017,
  author         = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  date           = {2017-11},
  journaltitle   = {IEEE Signal Processing Magazine},
  title          = {Deep reinforcement learning: A brief survey},
  doi            = {10.1109/{MSP}.2017.2743240},
  issn           = {1053-5888},
  number         = {6},
  pages          = {26--38},
  volume         = {34},
  abstract       = {Deep reinforcement learning (DRL) is poised to revolutionize the field of artificial intelligence (AI) and represents a step toward building autonomous systems with a higherlevel understanding of the visual world. Currently, deep learning is enabling reinforcement learning (RL) to scale to problems that were previously intractable, such as learning to play video games directly from pixels. DRL algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of RL, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep RL, including the deep Q-network (DQN), trust region policy optimization (TRPO), and asynchronous advantage actor critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via RL. To conclude, we describe several current areas of research within the field.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ReinfoLrng},
  timestamp      = {2020-02-28 03:33},
}

@Article{Bertsekas-2018,
  author         = {Bertsekas, Dimitri P.},
  date           = {2018-04-12},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Feature-Based Aggregation and Deep Reinforcement Learning: A Survey and Some New Implementations},
  url            = {https://arxiv.org/abs/1804.04577},
  abstract       = {In this paper we discuss policy iteration methods for approximate solution of a finite-state discounted Markov decision problem, with a focus on feature-based aggregation methods and their connection with deep reinforcement learning schemes. We introduce features of the states of the original problem, and we formulate a smaller "aggregate" Markov decision problem, whose states relate to the features. We discuss properties and possible implementations of this type of aggregation, including a new approach to approximate policy iteration. In this approach the policy improvement operation combines feature-based aggregation with feature construction using deep neural networks or other calculations. We argue that the cost function of a policy may be approximated much more accurately by the nonlinear function of the features provided by aggregation, than by the linear function of the features provided by neural network-based reinforcement learning, thereby potentially leading to more effective policy improvement.},
  day            = {12},
  f1000-projects = {QuantInvest},
  groups         = {ML_ReinfoLrng},
  timestamp      = {2020-02-28 03:33},
}

@Article{Brown-Petrik-2018,
  author         = {Brown, Alexander and Petrik, Marek},
  date           = {2018-09-19},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Interpretable Reinforcement Learning with Ensemble Methods},
  url            = {https://arxiv.org/abs/1809.06995},
  abstract       = {We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methods.},
  day            = {19},
  f1000-projects = {QuantInvest},
  groups         = {ML_Interpretability, ML_ReinfoLrng},
  timestamp      = {2020-02-28 03:33},
}

@Article{Das-et-al-2010,
  author               = {Das, Sanjiv and Markowitz, Harry and Scheid, Jonathan and Statman, Meir},
  date                 = {2010-04},
  journaltitle         = {Journal of Financial and Quantitative Analysis},
  title                = {Portfolio Optimization with Mental Accounts},
  doi                  = {10.1017/s0022109010000141},
  issn                 = {1756-6916},
  pages                = {311--334},
  volume               = {45},
  abstract             = {We integrate appealing features of Markowitz's mean-variance portfolio theory (MVT) and Shefrin and Statman's behavioral portfolio theory (BPT) into a new mental accounting (MA) framework. Features of the MA framework include an MA structure of portfolios, a definition of risk as the probability of failing to reach the threshold level in each mental account, and attitudes toward risk that vary by account.

We demonstrate a mathematical equivalence between MVT, MA, and risk management using value at risk (VaR). The aggregate allocation across MA subportfolios is mean-variance efficient with short selling. Short-selling constraints on mental accounts impose very minor reductions in certainty equivalents, only if binding for the aggregate portfolio, offsetting utility losses from errors in specifying risk-aversion coefficients in MVT applications. These generalizations of MVT and BPT via a unified MA framework result in a fruitful connection between investor consumption goals and portfolio production.},
  citeulike-article-id = {13911149},
  citeulike-linkout-0  = {http://journals.cambridge.org/action/displayAbstract?fromPage=online and aid=7785697},
  citeulike-linkout-1  = {http://dx.doi.org/10.1017/s0022109010000141},
  groups               = {Goals_Invest, Mental_Accounting},
  owner                = {cristi},
  posted-at            = {2016-01-17 19:54:34},
  timestamp            = {2020-02-28 03:34},
}

@Article{Das-et-al-2018a,
  author         = {Das, Sanjiv Ranjan and Ostrov, Daniel N. and Radhakrishnan, Anand and Srivastav, Deep},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Dynamic Portfolio Allocation in Goals-Based Wealth Management},
  doi            = {10.2139/ssrn.3211951},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3211951},
  abstract       = {Given any set of exogenously provided efficient portfolios, we develop a dynamic programming algorithm that constructs an optimal portfolio trading strategy to maximize the probability of attaining an investor specified goal wealth at the end of a designated timeframe. Our algorithm can also accommodate periodic infusions or withdrawals of any size with no degradation in runtime performance. We explore how the terminal wealth distribution is sensitive to restrictions on the segment of the portfolio efficient frontier made available to the investor. Because our algorithm optimal strategy is on the efficient frontier, allowed to depend on the investor wealth, and allowed to depend on the investor individual goals and specifications, we show that it soundly beats the performance of target date funds for attaining investors goals. These optimal goals-based wealth management strategies are useful for modern day FinTech offerings, both advisor-driven or robo-driven.},
  f1000-projects = {QuantInvest},
  groups         = {Goals_Invest, Invest_Dynamic, Individ_Goals, FinTech_WealthTech},
  timestamp      = {2020-02-28 03:34},
}

@Article{Dempster-et-al-2015a,
  author               = {Dempster, M. A. H. and Kloppers, Dwayne and Osmolovskiy, Igor and Medova, Elena and Ustinov, Philipp},
  date                 = {2015-03},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Life Cycle Goal Achievement or Portfolio Volatility Reduction?},
  url                  = {https://ssrn.com/abstract=2586178},
  abstract             = {This paper is concerned with the use of currently available technology to provide individuals, financial advisors and pension fund financial planners with detailed prospective financial plans tailored to an individual's financial goals and obligations. By taking account of all prospective cash flows of an individual, including servicing current liabilities, and simultaneously optimizing prospective spending, saving, asset allocation, tax, insurance, etc. using dynamic stochastic optimization, the paper addresses the question of the title by comparing the results of such a goal-based fully dynamic strategy with representative current best practices of the financial advisory industry. These include piecemeal fixed allocation portfolios for specific goals, target-date retirement funds and fixed real income post-retirement financial products, all using Markowitz mean variance optimization based on the very general goal of minimizing portfolio volatility for a specific portfolio expected return over a finite horizon. Making use of the same data and market calibrated Monte Carlo stochastic simulation for all the alternative portfolio strategies, we find that flexibility turns out to be of key importance to individuals for both portfolio and spending decisions. The performance of the adaptive dynamic goal-based portfolio strategy is found to be far superior to all the industry's Markowitz-based approaches. Superiority is measured here by the certainty equivalent increase in expected utility of individual lifetime consumption (gamma) and the extra initial capital required by an individual to put the dominated strategy on the same expected utility footing as the optimal dynamic strategy (initial capital gap). These empirical results should put paid to the commonly held view amongst finance professionals that the extra complexity of holistic dynamic stochastic models is not worth the marginal extra value obtained from their employment. We hope that such approaches implemented in currently available technologies will rapidly find acceptance by individuals, financial advisors and pension funds to the genuine benefit of individual investors.},
  citeulike-article-id = {13911283},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2586178},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2586922code34782.pdf?abstractid=2586178 and mirid=1},
  day                  = {29},
  groups               = {Goals_Invest, Individ_Goals},
  owner                = {zkgst0c},
  posted-at            = {2016-06-15 19:24:04},
  timestamp            = {2020-02-28 03:34},
}

@Article{Dempster-et-al-2016,
  author               = {Dempster, M. A. H. and Kloppers, Dwayne and Medova, Elena and Osmolovsky, Igor and Ustinov, Philipp},
  date                 = {2016-01},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Lifecycle Goal Achievement or Portfolio Volatility Reduction?},
  doi                  = {10.3905/jpm.2016.42.2.099},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {99--117},
  volume               = {42},
  abstract             = {This article is concerned with the use of currently available technology to offer individuals, financial advisors, and pension fund financial planners detailed prospective financial plans tailored to an individual's financial goals and obligations. By taking account of all an individual's prospective cash flows, including servicing current liabilities, and simultaneously optimizing prospective spending, saving, asset allocation, tax, and insurance, etc. using dynamic stochastic optimization, the authors compare the results of their goal-based fully dynamic strategy with the financial advisory industry's representative current best practices.

These include piecemeal fixed-allocation portfolios for specific goals, target-date retirement funds, and fixed real-income post-retirement financial products, all using Markowitz mean-variance optimization based on the very general goal of minimizing portfolio volatility for a specific portfolio expected return over a finite horizon. Making use of the same data and marketcalibrated Monte Carlo stochastic simulation for all the alternative portfolio strategies, the authors find that flexibility is of key importance for both individual portfolio and spending decisions. The authors measure superiority by the certainty-equivalent increase in expected utility of individual lifetime consumption (gamma) and the extra initial capital required by an individual to put the dominated strategy on the same expected-utility footing as the optimal dynamic strategy (initial capital gap).

They find that the adaptive dynamic goal-based portfolio strategy's performance is far superior to all the industry's Markowitz-based approaches. These empirical results should put paid to the commonly held view that the extra complexity of holistic dynamic stochastic models is not worth the marginal extra value obtained from their use.},
  citeulike-article-id = {13922845},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2016.42.2.099},
  groups               = {Goals based investing, Goals_Invest, Individ_Goals, PortfOptim_Goals},
  owner                = {zkgst0c},
  posted-at            = {2016-01-31 16:15:14},
  timestamp            = {2020-02-28 03:34},
}

@Article{Dempster-Medova-2011,
  author       = {Dempster, M. A. H. and Medova, E. A.},
  date         = {2011},
  journaltitle = {British Actuarial Journal},
  title        = {Asset Liability Management for Individual Households},
  number       = {2},
  url          = {https://pdfs.semanticscholar.org/f869/850af74ffc248e1370a8a119cca7e36cb2d7.pdf},
  volume       = {16},
  abstract     = {Personal finance is a challenging topic which can benefit from a scientific approach to individual financial planning. This paper presents an individual asset liability management (iALM) model for life cycle planning which uses the methodology of dynamic stochastic optimisation and incorporates ideas from both classical and behavioural finance. Its implementation is in the form of a decision support tool for use by financial advisers or wealth managers.

The investment universe is given by a set of indices for major asset classes and their returns are simulated forward over the lifetime of a household. On the liability side the foreseen cash flows of incomes and outgoings are simulated and punctuated by life events such as illness and death. The household's utility function is constructed for each time period over a range of monetary values in terms of household financial goals and preferences.

Taxes and pension savings are treated using the tax shielded saving accounts specific to a national jurisdiction in terms of constraints in the optimisation sub-models. The paper goes on to present an analysis of iALM model recommendations for a representative UK household, together with an evaluation of the sensitivity of the financial plan generated to changes in market environments such as the 2007-9 crisis.

The promise of this new technology is to bring modern decision support tools to individual investors in order to facilitate custom designed consumption, savings and investment policies},
  groups       = {Goals based investing, Goals_Invest, ALM_Individual},
  howpublished = {Available at http://www.cambridge-systems.com/content/CSAHouseholdiALM-BAJ16-2.pdf},
  journal      = {British Actuarial Journal},
  owner        = {zkgst0c},
  timestamp    = {2020-02-28 03:34},
  year         = {2011},
}

@Article{Ge-2018a,
  author         = {Ge, Wei},
  date           = {2018-03-31},
  journaltitle   = {The Journal of Alternative Investments},
  title          = {Alternatives to alternative assets:assessing S\&P 500 index option strategies as hedge fund replacements},
  doi            = {10.3905/jai.2018.20.4.069},
  issn           = {1520-3255},
  number         = {4},
  pages          = {69--80},
  volume         = {20},
  abstract       = {Hedge funds should deliver good risk-adjusted returns and provide diversification for investors portfolios. However, the performance of hedge funds has deteriorated in recent years and many investors may not find investment in hedge funds suitable, due to the high costs, illiquidity, and lack of transparency. The focus of this study is to examine whether equity index option-based strategies may deliver the same coveted ideal hedge fund return characteristics, that is, good risk-adjusted returns, mitigated risk measures, and added diversification. A total of seven SandP 500 Index-based generic option-writing strategies, named CP100 to CP0, are analyzed and compared with a comprehensive set of fourteen Credit Suisse hedge fund indexes using volatility or beta as matching metrics. The three series of CP100, CP80, and CP50, can be viewed as a solid suite of liquid alternatives to hedge funds. They match the fourteen Credit Suisse hedge fund indexes on beta levels, but deliver better risk-adjusted returns. Their annual returns and risk measures are better than or on par with the hedge fund indexes, except for the Credit Suisse Global Macro Index, which may be matched by a levered version of the CP100 option strategy. This suite of three option strategies are transparent, liquid, inexpensive, and easy to implement. They can achieve the same goals of hedge funds: attractive risk-adjusted returns, subdued risks, and added diversification. They may play multiple and important roles in institutional investors portfolios.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Goals_Invest, Risk_Liquidity, Hedge_Funds},
  timestamp      = {2020-02-28 03:34},
}

@Article{Deng-et-al-2017,
  author               = {Deng, Yue and Bao, Feng and Kong, Youyong and Ren, Zhiquan and Dai, Qionghai},
  date                 = {2017-03},
  journaltitle         = {IEEE Transactions on Neural Networks and Learning Systems},
  title                = {Deep Direct Reinforcement Learning for Financial Signal Representation and Trading},
  doi                  = {10.1109/tnnls.2016.2522401},
  issn                 = {2162-237X},
  number               = {3},
  pages                = {653--664},
  volume               = {28},
  abstract             = {Can we train the computer to beat experienced traders for financial assert trading? In this paper, we try to address this challenge by introducing a recurrent deep neural network (NN) for real-time financial signal representation and trading. Our model is inspired by two biological-related learning concepts of deep learning (DL) and reinforcement learning (RL). In the framework, the DL part automatically senses the dynamic market condition for informative feature learning. Then, the RL module interacts with deep representations and makes trading decisions to accumulate the ultimate rewards in an unknown environment. The learning system is implemented in a complex NN that exhibits both the deep and recurrent structures. Hence, we propose a task-aware backpropagation through time method to cope with the gradient vanishing issue in deep training. The robustness of the neural system is verified on both the stock and the commodity future markets under broad testing conditions.},
  citeulike-article-id = {14503702},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/tnnls.2016.2522401},
  groups               = {ML_ReinfoLrng},
  posted-at            = {2017-12-16 11:44:23},
  timestamp            = {2020-02-28 03:35},
}

@TechReport{Fischer-2018,
  author         = {Fischer, Thomas G.},
  date           = {2018},
  institution    = {Friedrich-Alexander University},
  title          = {Reinforcement learning in financial markets - a survey},
  url            = {https://www.econstor.eu/handle/10419/183139},
  abstract       = {The advent of reinforcement learning (RL) in financial markets is driven by several advantages inherent to this field of artificial intelligence. In particular, RL allows to combine the "prediction" and the "portfolio construction" task in one integrated step, thereby closely aligning the machine learning problem with the objectives of the investor. At the same time, important constraints, such as transaction costs, market liquidity, and the investor's degree of risk-aversion, can be conveniently taken into account. Over the past two decades, and albeit most attention still being devoted to supervised learning methods, the RL research community has made considerable advances in the finance domain. The present paper draws insights from almost 50 publications, and categorizes them into three main approaches, i.e., critic-only approach, actor-only approach, and actor-critic approach. Within each of these categories, the respective contributions are summarized and reviewed along the representation of the state, the applied reward function, and the action space of the agent. This cross-sectional perspective allows us to identify recurring design decisions as well as potential levers to improve the agent's performance. Finally, the individual strengths and weaknesses of each approach are discussed, and directions for future research are pointed out.},
  f1000-projects = {QuantInvest},
  groups         = {ML_PerfMetrics, ML_ReinfoLrng},
  publisher      = {Friedrich-Alexander University},
  timestamp      = {2020-02-28 03:35},
}

@Article{Halperin-Feldshteyn-2018,
  author         = {Halperin, Igor and Feldshteyn, Ilya},
  date           = {2018-05-07},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Market Self-Learning of Signals, Impact and Optimal Trading: Invisible Hand Inference with Free Energy (Or, How We Learned to Stop Worrying and Love Bounded Rationality)},
  url            = {https://ssrn.com/abstract=3174498},
  abstract       = {We present a simple model of a non-equilibrium self-organizing market where asset prices are partially driven by investment decisions of a bounded-rational agent. The agent acts in a stochastic market environment driven by various exogenous "alpha" signals, agent's own actions (via market impact), and noise. Unlike traditional agent-based models, our agent aggregates all traders in the market, rather than being a representative agent. Therefore, it can be identified with a bounded-rational component of the market itself, providing a particular implementation of an Invisible Hand market mechanism. In such setting, market dynamics are modeled as a fictitious self-play of such bounded-rational market-agent in its adversarial stochastic environment. As rewards obtained by such self-playing market agent are not observed from market data, we formulate and solve a simple model of such market dynamics based on a neuroscience-inspired Bounded Rational Information Theoretic Inverse Reinforcement Learning (BRIT-IRL). This results in effective asset price dynamics with a non-linear mean reversion - which in our model is generated dynamically, rather than being postulated. We argue that our model can be used in a similar way to the Black-Litterman model. In particular, it represents, in a simple modeling framework, market views of common predictive signals, market impacts and implied optimal dynamic portfolio allocations, and can be used to assess values of private signals. Moreover, it allows one to quantify a "market-implied" optimal investment strategy, along with a measure of market rationality. Our approach is numerically light, and can be implemented using standard off-the-shelf software such as TensorFlow.},
  day            = {7},
  f1000-projects = {QuantInvest},
  groups         = {ML_ReinfoLrng, ML_AssetPricing, ML_InvestSelect},
  timestamp      = {2020-02-28 03:35},
}

@Article{Huang-2018,
  author         = {Huang, Chien Yi},
  date           = {2018-07-08},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Financial Trading as a Game: A Deep Reinforcement Learning Approach},
  url            = {https://arxiv.org/abs/1807.02787},
  abstract       = {An automatic program that generates constant profit from the financial market is lucrative for every market practitioner. Recent advance in deep reinforcement learning provides a framework toward end-to-end training of such trading agent. In this paper, we propose an Markov Decision Process (MDP) model suitable for the financial trading task and solve it with the state-of-the-art deep recurrent Q-network (DRQN) algorithm. We propose several modifications to the existing learning algorithm to make it more suitable under the financial trading setting, namely 1. We employ a substantially small replay memory (only a few hundreds in size) compared to ones used in modern deep reinforcement learning algorithms (often millions in size.) 2. We develop an action augmentation technique to mitigate the need for random exploration by providing extra feedback signals for all actions to the agent. This enables us to use greedy policy over the course of learning and shows strong empirical performance compared to more commonly used epsilon-greedy exploration. However, this technique is specific to financial trading under a few market assumptions. 3. We sample a longer sequence for recurrent neural network training. A side product of this mechanism is that we can now train the agent for every T steps. This greatly reduces training time since the overall computation is down by a factor of T. We combine all of the above into a complete online learning algorithm and validate our approach on the spot foreign exchange market.},
  day            = {8},
  f1000-projects = {QuantInvest},
  groups         = {ML_ReinfoLrng},
  timestamp      = {2020-02-28 03:35},
}

@Article{Irlam-2018,
  author         = {Irlam, Gordon},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Financial planning via deep reinforcement learning AI},
  doi            = {10.2139/ssrn.3201703},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3201703},
  abstract       = {This paper introduces AIPlanner, a financial planner based upon deep reinforcement learning. AIPlanner provides an investment and consumption strategy intended to optimize lifetime well-being. The results of AIPlanner are very close to the precise analytical solution, as well as to the precise solution computed using stochastic dynamic programming. Deep reinforcement learning is additionally capable of delivering results for far more complicated and realistic financial models that other approaches can't handle. As an example of this capability, a bond model that includes both a yield curve and time varying interest rates is employed. Compared to other popular approaches, in one reasonable scenario, AIPlanner was found to effectively deliver approximately 1,000 to 8,000 of additional consumption per year.},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_YieldCurve, ML_ReinfoLrng},
  timestamp      = {2020-02-28 03:35},
}

@Article{Janssen-et-al-2013,
  author               = {Janssen, Ronald and Kramer, Bert and Boender, Guus},
  date                 = {2013-04},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Life Cycle Investing: From Target-Date to Goal-Based Investing},
  doi                  = {10.3905/jwm.2013.16.1.023},
  issn                 = {1520-4154},
  number               = {1},
  pages                = {23--32},
  volume               = {16},
  abstract             = {The authors propose the use of goal based investing- or private ALM, as they prefer to call it- to tailor a dynamic investment strategy to the needs of individual clients. They argue that this approach is superior to the - one-size-fits-all,- target-date-oriented static allocation path used in most current life cycle funds. They present the two pillars of their approach: the methodology for obtaining financial and economic scenarios, and the methodology of the goal-oriented dynamic allocation strategy. This approach reduces the risks and improves the feasibility of meeting the clients' goals.},
  citeulike-article-id = {13937246},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2013.16.1.023},
  groups               = {Goals based investing, Goals_Invest, Individ_Goals},
  owner                = {zkgst0c},
  posted-at            = {2016-02-20 19:30:28},
  timestamp            = {2020-02-28 03:36},
}

@Article{Kim-et-al-2019d,
  author         = {Kim, Woo Chang and Kwon, Do-Gyun and Lee, Yongjae and Kim, Jang Ho and Lin, Changle},
  date           = {2019-10-31},
  journaltitle   = {Quantitative Finance},
  title          = {Personalized goal-based investing via multi-stage stochastic goal programming},
  doi            = {10.1080/14697688.2019.1662079},
  issn           = {1469-7688},
  pages          = {1--12},
  urldate        = {2019-11-05},
  abstract       = {In this paper, we propose a goal-based investment model that is suitable for personalized wealth management. The model only requires a few intuitive inputs such as size of wealth, investment amount, and consumption goals from individual investors. In particular, a priority level can be assigned to each consumption goal and the model provides a holistic solution based on a sequential approach starting with the highest priority. This allows strict prioritization by maximizing the probability of achieving higher priority goals that are not affected by goals with lower priorities. Furthermore, the proposed model is formulated as a linear program that efficiently finds the optimal financial plan. With its simplicity, flexibility, and computational efficiency, the proposed goal-based investment model provides a new framework for automated investment management services.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Goals_Invest, Individualized_Invest},
  timestamp      = {2020-02-28 03:36},
}

@Article{Kopa-et-al-2018,
  author               = {Kopa, Milos and Moriggia, Vittorio and Vitali, Sebastiano},
  date                 = {2018},
  journaltitle         = {Annals of Operations Research},
  title                = {Individual optimal pension allocation under stochastic dominance constraints},
  doi                  = {10.1007/s10479-016-2387-x},
  number               = {1-2},
  pages                = {255--291},
  volume               = {260},
  abstract             = {An individual investor has to decide how to allocate his/her savings from a retirement perspective. This problem covers a long-term horizon. In this paper we consider a 40-year horizon formulating a multi-criteria multistage program with stochastic dominance constraints in an intermediate stage and in the final stage. As we are dealing with a real problem and we have formulated the model in cooperation with a commercial Italian bank, the intermediate stage corresponds to a possible withdrawal allowed by the Italian pension system. The sources of uncertainty considered are: the financial returns, the interest rate evolution, the investor's salary process and a considerable withdrawal event. We include a set of portfolio constraints according to the pension plan regulation. The objective of the model is to minimize the Average Value at Risk Deviation measure and to satisfy wealth goals. Three different wealth target formulations are considered: a deterministic wealth target (i.e. a comparison between the accumulated average wealth and a fixed threshold) and two stochastic dominance relations-the first order and the second order-introducing a benchmark portfolio and then requiring the optimal portfolio to dominate the benchmark. In particular, we prove that solutions obtained under stochastic dominance constraints ensure a safer allocation while still guaranteeing good returns. Moreover, we show how the withdrawal event affects the solution in terms of allocation in each of the three frameworks. Finally, the sensitivity and convergence of the stochastic solutions and computational issues are investigated.},
  citeulike-article-id = {14514203},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10479-016-2387-x},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10479-016-2387-x},
  groups               = {Goals_Invest},
  posted-at            = {2018-01-09 19:57:59},
  publisher            = {Springer US},
  timestamp            = {2020-02-28 03:36},
}

@Article{Maclean-et-al-2005,
  author               = {Maclean and Ziemba, William T. and Li, Yuming},
  date                 = {2005-08},
  journaltitle         = {Quantitative Finance},
  title                = {Time to wealth goals in capital accumulation},
  doi                  = {10.1080/14697680500149552},
  number               = {4},
  pages                = {343--355},
  volume               = {5},
  abstract             = {This paper considers the problem of investment of capital in risky assets in a dynamic capital market in continuous time. The model controls risk, and in particular the risk associated with errors in the estimation of asset returns. The framework for investment risk is a geometric Brownian motion model for asset prices, with random rates of return. The information filtration process and the capital allocation decisions are considered separately. The filtration is based on a Bayesian model for asset prices, and an (empirical) Bayes estimator for current price dynamics is developed from the price history. Given the conditional price dynamics, investors allocate wealth to achieve their financial goals efficiently over time. The price updating and wealth reallocations occur when control limits on the wealth process are attained. A Bayesian fractional Kelly strategy is optimal at each rebalancing, assuming that the risky assets are jointly lognormal distributed. The strategy minimizes the expected time to the upper wealth limit while maintaining a high probability of reaching that goal before falling to a lower wealth limit. The fractional Kelly strategy is a blend of the log-optimal portfolio and cash and is equivalently represented by a negative power utility function, under the multivariate lognormal distribution assumption. By rebalancing when control limits are reached, the wealth goals approach provides greater control over downside risk and upside growth. The wealth goals approach with random rebalancing times is compared to the expected utility approach with fixed rebalancing times in an asset allocation problem involving stocks, bonds, and cash.},
  citeulike-article-id = {13990197},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697680500149552},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697680500149552},
  day                  = {1},
  groups               = {Goals based investing, Goals_Invest},
  owner                = {cristi},
  posted-at            = {2016-03-29 11:25:12},
  publisher            = {Routledge},
  timestamp            = {2020-02-28 03:36},
}

@Article{Marchioni-et-al-2015,
  author               = {Marchioni, Ursula and Antropova, Sofia and McNaught, Catherine},
  date                 = {2015-05},
  journaltitle         = {The Journal of Index Investing},
  title                = {Smart Beta Strategies as Outcome-Oriented Solutions in the Equity Space},
  doi                  = {10.3905/jii.2015.6.1.065},
  issn                 = {2154-7238},
  number               = {1},
  pages                = {65--78},
  volume               = {6},
  abstract             = {As index investing continues to grow, we are witnessing an expansion of what the beta continuum encompasses. During the 2000 dot-com bubble and the 2007-2008 financial crisis, market-capitalization weighted strategies revealed some of their limitations. As a result, increasing attention is being devoted to index solutions that go beyond the traditional space-often referred to as "smart beta."

In this article, we focus our attention on a sample of beta strategies within the equity space. By analyzing each beta's benefits and limitations, we highlight their ultimate nature as outcome-orientated solutions. We believe the results of our analysis encourage investors to think about beta as a range of complementary tools that should be used to actively rotate portfolios from one indexed strategy to another, based on investors' goals and on prevailing market conditions.},
  citeulike-article-id = {14339549},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jii.2015.6.1.065},
  groups               = {Goals_Invest, Invest_SmartBeta},
  posted-at            = {2017-04-19 12:03:30},
  timestamp            = {2020-02-28 03:36},
}

@Article{Mladina-2016,
  author               = {Mladina, Peter},
  date                 = {2016-04},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Optimal Lifetime Asset Allocation with Goals-Based, Lifecycle Glide Paths},
  doi                  = {10.3905/jwm.2016.19.1.010},
  issn                 = {1534-7524},
  number               = {1},
  pages                = {10--22},
  volume               = {19},
  citeulike-article-id = {14030193},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2016.19.1.010},
  groups               = {Goals based investing, Goals_Invest, Individ_Goals},
  owner                = {cristi},
  posted-at            = {2016-05-08 23:24:43},
  timestamp            = {2020-02-28 03:36},
}

@TechReport{MLWIM-2013,
  author               = {{MLWM Institute}},
  date                 = {2013},
  institution          = {Merrill Lynch Wealth Management},
  title                = {Goals-Based Wealth Management: Helping you pursue personally meaningful goals},
  abstract             = {Our approach, Goals-Based Wealth Management (GBWM), is designed to help you pursue personally meaningful goals. The GBWM approach helps you and your Advisor work together to define your goals and priorities, and to determine how to make the most of your financial resources. Your Advisor can then recommend appropriate solutions to help you achieve your goals, assist you as you track progress toward them and work with you to adjust your investment strategy, if necessary.},
  citeulike-article-id = {13911142},
  groups               = {Goals based investing, Goals_Invest, Individ_Goals, Goals_Priority, [nbkcbu3:]},
  howpublished         = {Available at http://www.ml.com/publish/content/application/pdf/gwmol/470599pm.pdf},
  owner                = {cristi},
  posted-at            = {2016-01-17 19:13:07},
  timestamp            = {2020-02-28 03:36},
}

@Article{Nevins-2004,
  author               = {Nevins, Daniel},
  date                 = {2004-01},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Goals-Based Investing},
  doi                  = {10.3905/jwm.2004.391053},
  issn                 = {1520-4154},
  number               = {4},
  pages                = {8--23},
  volume               = {6},
  abstract             = {This article examines opportunities to improve wealth management by combining traditional finance theory with the observations of behavioral finance. Areas of focus include risk measurement, risk profiling, and methods for managing behavioral biases. In the area of risk measurement, the author stresses the importance of capturing investor preferences and goals and proposes several measures that are consistent with this objective. The author also critiques common risk profiling techniques, advocating separate risk tolerance estimates for separate goals rather than an overall risk tolerance for each investor, noting that the total portfolio framework of traditional finance is inconsistent with investors' tendencies towards mental accounting. A better result may be achieved by linking individual strategies to a specific goal or goals. The author describes a process for implementing his recommendations through examples, considering the challenges of investing to meet current lifestyle expenses and investing for a fixed planning horizon. The article closes with a call to align investment strategy development with common investor goals, arguing that this will promote consistency between the investment principles of the practitioner and the perspective of the individual investor.},
  citeulike-article-id = {14343839},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2004.391053},
  groups               = {Goals based investing, Goals_Invest, Individ_Goals},
  posted-at            = {2017-04-24 11:25:55},
  timestamp            = {2020-02-28 03:36},
}

@TechReport{NorthernTrust-2015a,
  author               = {{Northern Trust Research}},
  date                 = {2015},
  institution          = {Northern Trust},
  title                = {An introduction to goals driven investing},
  url                  = {https://www.northerntrust.com/documents/white-papers/wealth-management/goals-driven-investing.pdf},
  citeulike-article-id = {13937232},
  groups               = {Goals based investing, Goals_Invest, Individ_Goals},
  howpublished         = {Available at https://www.northerntrust.com/documents/white-papers/wealth-management/goals-driven-investing.pdf},
  owner                = {zkgst0c},
  posted-at            = {2016-02-20 18:59:47},
  timestamp            = {2020-02-28 03:36},
}

@TechReport{Nuveen-2015,
  author               = {{Nuveen Investments Research}},
  date                 = {2015},
  institution          = {Nuveen Investments},
  title                = {Rethinking Portfolio Construction: Moving to a Goals-Based Approach},
  url                  = {https://www.nuveen.com/en-us/thinking/outcome-oriented-investing/outcome-oriented-investing},
  abstract             = {At Nuveen Investments, we believe it's time for investors to rethink the way they approach investing, moving beyond the traditional view that only stocks can provide portfolio growth and bonds generate income streams.

In an era of extraordinary choice and increasing accessibility of less-traditional strategies, we suggest a framework for portfolio construction that relates directly to individual investors' goals and objectives.

This framework aims to organize strategies based on the goals investors are seeking to achieve, which complements asset class- and peer groupbased categorizations available more broadly.},
  citeulike-article-id = {13937264},
  groups               = {Goals based investing, Goals_Invest, Individ_Goals},
  howpublished         = {Available at http://www.nuveen.com/Home/Documents/Default.aspx?fileId=65225},
  owner                = {zkgst0c},
  posted-at            = {2016-02-20 20:16:07},
  timestamp            = {2020-02-28 03:36},
}

@Article{Jiang-et-al-2017a,
  author               = {Jiang, Zhengyao and Xu, Dixing and Liang, Jinjun},
  date                 = {2017},
  journaltitle         = {arXiv Electronic Journal},
  title                = {A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem},
  eprint               = {1706.10059},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1706.10059},
  abstract             = {Financial portfolio management is the process of constant redistribution of a fund into different financial products. This paper presents a financial-model-free Reinforcement Learning framework to provide a deep machine learning solution to the portfolio management problem. The framework consists of the Ensemble of Identical Independent Evaluators (EIIE) topology, a Portfolio-Vector Memory (PVM), an Online Stochastic Batch Learning (OSBL) scheme, and a fully exploiting and explicit reward function. This framework is realized in three instants in this work with a Convolutional Neural Network (CNN), a basic Recurrent Neural Network (RNN), and a Long Short-Term Memory (LSTM). They are, along with a number of recently reviewed or published portfolio-selection strategies, examined in three back-test experiments with a trading period of 30 minutes in a cryptocurrency market. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. All three instances of the framework monopolize the top three positions in all experiments, outdistancing other compared trading algorithms. Although with a high commission rate of 0.25\% in the backtests, the framework is able to achieve at least 4-fold returns in 50 days.},
  citeulike-article-id = {14449203},
  citeulike-linkout-0  = {http://arxiv.org/abs/1706.10059},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1706.10059},
  day                  = {16},
  groups               = {Machine learning and investment strategies, ML_ReinfoLrng, ML_InvestSelect, DeepLearning_QWIM},
  posted-at            = {2017-10-12 00:17:25},
  timestamp            = {2020-02-28 03:36},
  year                 = {2017},
}

@Article{Kolm-Ritter-2019,
  author         = {Kolm, Petter N. and Ritter, Gordon},
  date           = {2019-01-31},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {Dynamic Replication and Hedging: A Reinforcement Learning Approach},
  doi            = {10.3905/jfds.2019.1.1.159},
  url            = {https://jfds.pm-research.com/content/1/1/159},
  abstract       = {The authors of this article address the problem of how to optimally hedge an options book in a practical setting, where trading decisions are discrete and trading costs can be nonlinear and difficult to model. Based on reinforcement learning (RL), a well-established machine learning technique, the authors propose a model that is flexible, accurate and very promising for real-world applications. A key strength of the RL approach is that it does not make any assumptions about the form of trading cost. RL learns the minimum variance hedge subject to whatever transaction cost function one provides. All that it needs is a good simulator, in which transaction costs and options prices are simulated accurately.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {ML_ReinfoLrng},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-02-28 03:36},
}

@Article{Li-2017,
  author               = {Li, Yuxi},
  date                 = {2017-09},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Deep Reinforcement Learning: An Overview},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1701.07274},
  abstract             = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.},
  citeulike-article-id = {14264155},
  citeulike-linkout-0  = {http://arxiv.org/abs/1701.07274},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1701.07274},
  day                  = {15},
  groups               = {ML_ReinfoLrng, ML_TransferLrng},
  posted-at            = {2017-12-11 20:19:42},
  timestamp            = {2020-02-28 03:36},
}

@Article{Liang-et-al-2018,
  author         = {Liang, Zhipeng and Jiang, Kangkang and Chen, Hao and Zhu, Junhao and Li, Yanran},
  date           = {2018-08-29},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Deep Reinforcement Learning in Portfolio Management},
  url            = {https://arxiv.org/abs/1808.09940v1},
  abstract       = {In this paper, we implement two state-of-art continuous reinforcement learning algorithms, Deep Deterministic Policy Gradient (DDPG) and Proximal Policy Optimization (PPO) in portfolio management. Both of them are widely-used in game playing and robot control. What's more, PPO has appealing theoretical propeties which is hopefully potential in portfolio management. We present the performances of them under different settings, including different learning rate, objective function, markets, feature combinations, in order to provide insights for parameter tuning, features selection and data preparation.},
  day            = {29},
  f1000-projects = {QuantInvest},
  groups         = {ML_FeatureSelection, ML_ReinfoLrng, DeepLearning_QWIM},
  timestamp      = {2020-02-28 03:36},
}

@Article{Li-et-al-2017h,
  author         = {Li, Hongjia and Wei, Tianshu and Ren, Ao and Zhu, Qi and Wang, Yanzhi},
  date           = {2017-10-10},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Deep Reinforcement Learning: Framework, Applications, and Embedded Implementations},
  url            = {https://arxiv.org/abs/1710.03792},
  abstract       = {The recent breakthroughs of deep reinforcement learning (DRL) technique in Alpha Go and playing Atari have set a good example in handling large state and actions spaces of complicated control problems. The DRL technique is comprised of (i) an offline deep neural network (DNN) construction phase, which derives the correlation between each state-action pair of the system and its value function, and (ii) an online deep Q-learning phase, which adaptively derives the optimal action and updates value estimates. In this paper, we first present the general DRL framework, which can be widely utilized in many applications with different optimization objectives. This is followed by the introduction of three specific applications: the cloud computing resource allocation problem, the residential smart grid task scheduling problem, and building HVAC system optimal control problem. The effectiveness of the DRL technique in these three cyber-physical applications have been validated. Finally, this paper investigates the stochastic computing-based hardware implementations of the DRL framework, which consumes a significant improvement in area efficiency and power consumption compared with binary-based implementation counterparts.},
  day            = {10},
  f1000-projects = {QuantInvest},
  groups         = {ML_ReinfoLrng},
  timestamp      = {2020-02-28 03:36},
}

@Article{MahdaviDamghani-2018,
  author         = {Mahdavi-Damghani, Babak},
  date           = {2018-07-31},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Machine Learning vs. Mathematical Finance: Opposition or Apposition? Thoughts Around Reconstructing Quantitative Finance a Decade after the Subprime Derivatives and Electronic Trading Flash Crashes In the Context of the Rise of Big Data and Data Science},
  url            = {https://ssrn.com/abstract=3223328},
  abstract       = {The rise of Big Data (BD) [13] and Data Science (DS) [14] along the aftermath of the financial crisis of 2009 as well as the multiple Flash Crashes of the early 2010s resulted in social uproars in the general population and ethical malaises in the scientific community [15, 7, 9, 8] which triggered noticeable changes in Quantitative Finance (QF). More specifically, QF was instructed to change [16, 17, 18] and become more realistic as opposed to more convenient. In terms of redefining the new models, we saw a significant wave favouring Machine Learning (ML) to take the leadership from Analysis and Probability Theory, a branch of the applied mathematical and computational sciences commonly known as traditional Mathematical Finance (MF) models. In this context we show how ML can allow us to completely revolutionize QF and how these changes of approach are as much mind boggling as too theoretical at this stage. But we will also see that ML can allow us to still enhance, transition out of, or rather add on to traditional MF for more practical, practitioners friendly models for a more realistic approach to QF. In this dual context, we introduce a revolutionary bottom up approach to algorithmic trading (AT) in the form of an original ecosystem of strategies [2]. We also continue opposing the two fields by introducing new methodologies to capture a broader elements of risk whether through harder science methodologies such as with Particle Filter (PF) [1] or via softer science approach through Psychology [7] taking as inspirational roots some of the common limitations of Reinforcement Learning and over-fitting in general. However the core of our original contribution is centered around reconciling the two fields and therefore apposing them. More specifically we take a look at few more humble DS and BD related problems such as exposing the risk associated to Cointelated pairs [9, 8] or in the context of their pairs trading [3]. We also address few Implied Volatility (IV) related problems like Anomaly Detection [10], Risk Factor Decomposition [11], Normalization [4], Reconciliation [5] and also see how BD is changing the way IV is modeled [6].},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Bubble_Crash, Invest_FinDeriv, ML_ReinfoLrng, ML_Text_QWIM},
  timestamp      = {2020-02-28 03:36},
}

@InCollection{Mousavi-et-al-2018,
  author         = {Mousavi, Seyed Sajad and Schukat, Michael and Howley, Enda},
  booktitle      = {Proceedings of SAI intelligent systems conference (intellisys) 2016},
  date           = {2018},
  title          = {Deep reinforcement learning: an overview},
  doi            = {10.1007/978-3-319-56991-8\_32},
  editor         = {Bi, Yaxin and Kapoor, Supriya and Bhatia, Rahul},
  isbn           = {978-3-319-56990-1},
  pages          = {426--440},
  publisher      = {Springer International Publishing},
  series         = {Lecture notes in networks and systems},
  volume         = {16},
  abstract       = {In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input. This chapter reviews the recent advances in deep reinforcement learning with a focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework.},
  f1000-projects = {QuantInvest},
  groups         = {DeepLearning_QWIM, ML_ReinfoLrng},
  issn           = {2367-3370},
  timestamp      = {2020-02-28 03:36},
}

@Article{Parham-2013,
  author               = {Parham, W. Jackson},
  date                 = {2013},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Introduction to Goals-Based Investing},
  citeulike-article-id = {13937245},
  groups               = {Goals based investing, Goals_Invest, Individ_Goals},
  howpublished         = {Available at http://www.windhamgs.com/wp-content/uploads/2013/30-0Jack0Parham.pdf},
  owner                = {zkgst0c},
  posted-at            = {2016-02-20 19:28:46},
  timestamp            = {2020-02-28 03:37},
}

@Article{Parker-2017,
  author               = {Parker, Franklin J.},
  date                 = {2017-04},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Knowing What to Worry About: A Goals Based Application of Fragility Detection Theory},
  doi                  = {10.3905/jwm.2017.2017.1.054},
  issn                 = {1534-7524},
  abstract             = {Drawing from the theory of nonlinear risk and fragility detection, we analyze which portfolio management attributes should be of most concern to the practitioner in a goal-based setting. We begin by constructing a risk measurement mechanism using a Gaussian stochastic Monte Carlo process. We then analyze which portfolio inputs are most sensitive to errors. Counterintuitively, we find that portfolio variance is of least concern, whereas factors affecting the future required minimum wealth level are of greatest concern. We show how goal-based practitioners get the most bang for their buck by focusing on the accuracy of their inflation projections (not often a first focus) and portfolio variance last (often the first focus). These results can help both the practitioner and researcher dedicate resources to answering the questions that are of most importance when a future goal is at stake.},
  citeulike-article-id = {14338556},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2017.2017.1.054},
  day                  = {07},
  groups               = {Goals based investing, Goals_Invest, Individ_Goals, Goals_Inflation},
  posted-at            = {2017-04-17 11:20:32},
  timestamp            = {2020-02-28 03:37},
}

@Article{Ransenberg-et-al-2012,
  author               = {Ransenberg, Dan and Hodges, Philip and Hunt, Andy},
  date                 = {2012-05},
  journaltitle         = {The Journal of Investing},
  title                = {LDI in a Risk Factor Framework},
  doi                  = {10.3905/joi.2012.21.2.105},
  issn                 = {1068-0896},
  number               = {2},
  pages                = {105--116},
  volume               = {21},
  abstract             = {Over 5 trillion in pension liabilities have been promised to individuals in close to 30,000 private and public pension plans across the U.S. Yet the vast majority of these pension plans do not fully incorporate their liabilities when making investment decisions. This article analyzes the fundamental risk factors common to a plan's assets and liabilities and proposes a risk factor framework for LDI.

At a very basic level, pension plans should focus on assets minus liabilities, that is, their surplus portfolio. They should understand what investment and actuarial risks are expected to be rewarded. The surplus portfolio should be constructed to have a diversified set of positive exposures to rewarded risk factors. Further, these surplus exposures should be scaled in accordance to the investor's conviction that the risk factor will be rewarded and to the extent to which it diversifies other risk factor exposures.

In this article, we demonstrate that pension plans that incorporate their pension liabilities into investment decisions in this risk factor framework can expect to achieve their funding status goals with considerably less uncertainty and better ensure that pension promises are kept.},
  citeulike-article-id = {13971087},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2012.21.2.105},
  groups               = {Factor based investing, Goals_Invest, LDI},
  owner                = {cristi},
  posted-at            = {2016-03-07 05:01:12},
  timestamp            = {2020-02-28 03:37},
}

@Article{Rubin-2015,
  author               = {Rubin, Matthew},
  date                 = {2015},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Asset Matters: How Goals-Based Allocation Drives Portfolio Positioning},
  url                  = {https://www.advisorperspectives.com/commentaries/2015/11/16/asset-matters-how-goals-based-allocation-drives-portfolio-positioning},
  abstract             = {The trend of goals-based allocation first took hold in the institutional world. Think of a company preparing for future pension outlays or a university endowment planning for a specific investment like a new campus building. Because institutional portfolio designers know the particular goal and the exact time horizon, they can design an allocation geared toward that outcome. This is known as liability-driven investing (LDI), and the process has become a standard practice for many institutions.

While a duration-targeted LDI portfolio may not suit the typical family, we can use the same concept to identify specific goals and timelines for clients, which then guide asset allocation proposals and portfolio construction. Here we share a few examples to bring the concept to life.},
  citeulike-article-id = {13937263},
  groups               = {Goals based investing, Goals_Invest, Individ_Goals},
  howpublished         = {Available at http://www.nb.com/pages/public/en-us/insights/asset-matters-how-goals-based-allocation-drives-portfolio-positioning.aspx},
  organization         = {Neuberger Berman},
  owner                = {zkgst0c},
  posted-at            = {2016-02-20 20:12:16},
  timestamp            = {2020-02-28 03:37},
  year                 = {2015},
}

@TechReport{SEI-2012,
  author               = {{SEI Investments Research}},
  date                 = {2012},
  institution          = {SEI Investments},
  title                = {Goal-based investing},
  url                  = {https://www.seic.com/Canada-Advisor/SEI-GoalsBasedInvesting-CA.pdf},
  citeulike-article-id = {13937240},
  groups               = {Goals based investing, Goals_Invest, Individ_Goals},
  howpublished         = {Available at https://www.seic.com/Canada-Advisor/SEI-GoalsBasedInvesting-CA.pdf},
  owner                = {zkgst0c},
  posted-at            = {2016-02-20 19:04:04},
  timestamp            = {2020-02-28 03:37},
}

@TechReport{Shalett-2014,
  author       = {Shalett, Lisa},
  date         = {2015},
  institution  = {Morgan Stanley Wealth Management},
  title        = {An Outcomes-Oriented Approach to Alternatives},
  url          = {https://www.morganstanley.com/wealth/investmentstrategies/pdfs/gic_approachtoalternatives_0214.pdf},
  abstract     = {Transformational forces are colliding in a way that necessitates a fresh approach to asset allocation guidance for alternative asset classes and strategies: the proliferation of lower-cost alternative investment formats; the normalization of interest rates; and the need to reintroduce alternatives to Financial Advisors and clients, many of whom in the past have been disillusioned by unfulfilled expectations, high fees, tax complexity and liquidity.

In this new outcomes-based approach, we have a navigation framework that is intuitive and tests for suitability through alignment with basic portfolio goals.

We also posit performance parameters that allow us to compare the trade-offs between alternative mutual funds/ ETFs and private offerings and suggest benchmarks that provide clients with a way to measure success.},
  groups       = {Goals based investing, BenchmarkInvest, Goals_Invest},
  organization = {Morgan Stanley Wealth Management},
  owner        = {zkgst0c},
  timestamp    = {2020-02-28 03:37},
}

@TechReport{Shalett-et-al-2017,
  author               = {Shalett, Lisa and Hunt, Daniel and Ye, Zi and Wang, Stephanie},
  date                 = {2017},
  institution          = {Morgan Stanley Wealth Management},
  title                = {Tax Efficiency: Getting to What You Need by Keeping More of What You Earn},
  url                  = {https://www.morganstanley.com/ideas/tax-efficiency},
  abstract             = {Taxes are a substantial drag on investment returns that compound over time. Because of this, even small reductions in tax costs can have enormous consequences for wealth accumulation. In an example drawn from this report, an improvement of 0.6\% per year in aftertax returns resulted in a remaining wealth difference of 75\% after 30 years of distributions. At a time when interest rates are low, and conservative and balanced portfolios don't deliver the returns they once did, investors can often improve their progress toward their financial goals by enhancing their tax efficiency. We outline strategies for doing so, ranging from the simple and commonly used to the underutilized, less well-known and more complex; evaluate each to ascertain its appropriateness for different investors; and assess their potential impact on investors' bottom lines.},
  citeulike-article-id = {14501877},
  groups               = {Taxes and investment strategies, Goals_Invest, Invest_Tax, Individ_Tax},
  posted-at            = {2017-12-12 22:35:04},
  timestamp            = {2020-02-28 03:37},
  year                 = {2017},
}

@Article{Shively-2015,
  author               = {Shively, Thomas},
  date                 = {2015},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Goals-Based Asset Allocation in an Era of Financial Repression},
  url                  = {https://www.advisorperspectives.com/commentaries/2015/09/25/goals-based-asset-allocation-in-an-era-of-financial-repression},
  abstract             = {Goals-based asset allocation seeks to align our total portfolio, including financial and nonfinancial assets, with our personal goals and our human way of thinking about risk.

The framework combines attributes of modern portfolio theory and behavioral finance in a common-sense manner, and is extremely flexible at the individual level.

Focusing the investor, and the portfolio, on meeting the investor's short- and long-term goals should reduce the tendency toward counterproductive reactions to market volatility.

This approach may be especially effective in the current environment of financial repression, where market valuations are intentionally distorted for policy purposes.},
  citeulike-article-id = {13937241},
  groups               = {Goals based investing, Goals_Invest, Individ_Goals},
  howpublished         = {Available at http://www.advisorperspectives.com/commentaries/20150925-eaton-vance-goals-based-asset-allocation-in-an-era-of-financial-repression},
  owner                = {zkgst0c},
  posted-at            = {2016-02-20 19:07:20},
  timestamp            = {2020-02-28 03:37},
}

@TechReport{Suryanarayanan-et-al-2014,
  author      = {Suryanarayanan, Raghu and Hentschel, Ludger and Panzano, Giulio and Schneider, Dan},
  date        = {2014},
  institution = {MSCI},
  title       = {Goal-Based Asset Allocation in WealthBench},
  abstract    = {MSCI recently introduced a goal-based asset allocation framework in WealthBench, MSCI's analytics and investment planning tool for financial advisors. This framework allows financial advisors to design asset allocation plans based on a client's specific financial goals.

Goal-based asset allocation provides the following key potential benefits to financial advisors and their clients:

1. The ability to identify an individual investor's specific financial goal -- such as retirement or education -- in detail, including the client's specified acceptable levels of shortfall risk for each goal and the desired trade-off between goals.

2. Since different investors have different goals, their asset allocation plans will differ.

3. The intuitive Asset-Liability methodology of the framework allows financial advisors to compare an investor's assets values against the funding requirements of future goals, at individual goal and overall investment plan levels.

4. The methodology can be used to design intuitive portfolios that differ from portfolios created using traditional optimization methods. For example, goal-based allocations naturally provide for less risky portfolios as the investor progresses towards high-priority goals like retirement.

This paper describes MSCI's goal-based asset allocation framework used in WealthBench and illustrates its benefits based on a detailed example. The goal-based methodology follows an asset and liability management approach to asset allocation, incorporating insights from Sharpe's (2007) lockbox separation approach. The identification and modeling of the individual investor's financial goals are central to the methodology.

Goals represent the liabilities for the individual investor. The amount required to fund each goal and the contributing assets, including future income and savings, are valued in present terms. Thus, the financial advisor visualizes the client's overall plan as a set of balance sheets, one for each goal and its associated assets.

Of course, these balance sheets can also be aggregated into an overall financial assessment of the client's net worth. The wealth planner finds the allocation of wealth over the life-cycle that funds each goal at the lowest initial cost, given an individual's desired trade-off between financial goals. The resulting dynamic asset allocation unveils a glide-path specific to the identified goals and priorities.

Finding optimal asset allocations for a goals-based financial plan is more demanding than standard methods. This is in part because each investor has different goals and hence a different optimal allocation. In addition, each investor's optimal allocation is a collection of goal-specific optimal allocations.

To overcome this hurdle, MSCI's goal-based methodology builds on the simulation framework that has been the heart of WealthBench since its inception. The simulation framework naturally accommodates an individual's specific financial goals that are unevenly distributed over the individual's life-cycle.

Another key benefit includes the ability to handle relevant real-life complexities in wealth planning, such as different sources of income, separate treatment for tax regimes affecting different assets, and special funding vehicles like trusts. In addition, the simulation framework facilitates scenario and sensitivity analysis for the overall investment plan and for individual financial goals.},
  groups      = {Goals based investing, Goals_Invest, Retire_Tax, Individ_Goals, Goals_Priority},
  owner       = {zkgst0c},
  timestamp   = {2020-02-28 03:37},
  year        = {2014},
}

@Article{Towle-2015,
  author               = {Towle, Margaret},
  date                 = {2015},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Impact Investing and Goals-Based Financial Planning},
  citeulike-article-id = {13937256},
  groups               = {Goals based investing, Goals_Invest},
  howpublished         = {Available at https://www.imca.org/sites/default/files/current-issues/IWM/IWM15NovDecImpactInvesting.pdf},
  owner                = {zkgst0c},
  posted-at            = {2016-02-20 19:52:55},
  timestamp            = {2020-02-28 03:37},
}

@TechReport{UBSCIO-2015,
  author               = {{UBSWM Research}},
  date                 = {2015},
  institution          = {UBS Wealth Management},
  title                = {Goals-based wealth management: Liquidity, longevity, and legacy},
  url                  = {https://www.ubs.com/content/dam/WealthManagementAmericas/documents/liquidity-longevity-legacy-uhnw.pdf?},
  abstract             = {The last two decades have produced a wealth of data indicating that investors fail on two levels: their buy/sell decisions detract value, on average, and they don't invest in a way that maximizes their likelihood of reaching their primary goals.

These failures might be due to behavioral biases that make it difficult to stick with modern portfolio theory or flaws in the framework itself, but for whatever reason the traditional approach to investing simply hasn't worked.

We believe goals-based wealth management is the solution.},
  citeulike-article-id = {13911141},
  groups               = {Goals based investing, Characteristics and return prediction, Goals_Invest, Individ_Goals, Individ_Longevity, Annuities},
  howpublished         = {Available at https://www.ubs.com/content/dam/WealthManagementAmericas/documents/your-wealth-and-life-1Q-2015-goals-based-wealth-management.pdf},
  owner                = {cristi},
  posted-at            = {2016-01-17 18:57:30},
  timestamp            = {2020-02-28 03:37},
}

@Article{Uhl-Rohner-2018,
  author         = {Uhl, Matthias W. and Rohner, Philippe},
  date           = {2018-02},
  journaltitle   = {Finance Research Letters},
  title          = {The compensation portfolio},
  doi            = {10.1016/j.frl.2018.02.023},
  issn           = {1544-6123},
  url            = {http://linkinghub.elsevier.com/retrieve/pii/S1544612317305299},
  abstract       = {We successfully show that it is possible to optimize both for risk and for asset allocation without compromising the optimization of individual goals by introducing the novel concept of a compensation portfolio. Therefore, we solve for the global vs. local optimization paradox by bridging Modern Portfolio Theory (MPT) and Behavioral Portfolio Theory (BPT).},
  f1000-projects = {QuantInvest},
  groups         = {Goals_Invest},
  timestamp      = {2020-02-28 03:37},
}

@Article{Wang-et-al-2011b,
  author               = {Wang, Hungjen and Suri, Anil and Laster, David and Almadi, Himanshu},
  date                 = {2011-04},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Portfolio Selection in Goals-Based Wealth Management},
  doi                  = {10.3905/jwm.2011.14.1.055},
  issn                 = {1520-4154},
  number               = {1},
  pages                = {55--65},
  volume               = {14},
  abstract             = {The authors propose an incremental step toward combining the insights of modern portfolio theory with some of the propensities documented in the literature on behavioral finance. They develop a goals-based wealth management approach that finds a specific subportfolio to address each of an investor's goals and then derive the least-cost solution. T

hey relate the closed-form solution for the one-period, two-asset problem to the mean-variance efficient frontier. Consistent with the - lockbox separation- concept proposed by Sharpe, they demonstrate that a multiperiod goal, such as a retirement plan, can be viewed as a collection of single-period problems. Next, they extend their result to a market with many assets, where portfolios are exogenously given.

Finally, they illustrate the approach with a case study with multiple asset classes and multiperiod goals.},
  citeulike-article-id = {13937229},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2011.14.1.055},
  groups               = {Goals based investing, Goals_Invest, Goals_Priority},
  owner                = {zkgst0c},
  posted-at            = {2016-02-20 18:42:12},
  timestamp            = {2020-02-28 03:37},
}

@Article{Wang-Spinney-2017,
  author               = {Wang, Peng and Spinney, Jon},
  date                 = {2017-10},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Strategic Asset Allocation: Combining Science and Judgment to Balance Short-Term and Long-Term Goals},
  doi                  = {10.3905/jpm.2017.44.1.069},
  issn                 = {0095-4918},
  number               = {1},
  pages                = {69--82},
  volume               = {44},
  abstract             = {The authors build on traditional mean-variance optimization with a quantitative framework for combining the best of science and judgment in selecting an asset allocation for long-horizon investors such as endowments. The novelty of their approach lies in its ability to balance the desire for long-term returns with the need to manage short-term risk and funding constraints-important goals that are often in conflict. To reap the benefits of long-term risk premia, investors must be able to withstand occasional short-term painful drawdowns. The authors show how their unified approach can be used to examine how different combinations of asset classes, spending rates, and even alpha impact the policy portfolio over various planning horizons. The framework merges the science of portfolio optimization with a structure that informs sound judgment in determining an organization's strategic asset allocation and spending policies.},
  citeulike-article-id = {14468536},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2017.44.1.069},
  day                  = {31},
  groups               = {SAA, Goals_Invest, Individ_Goals, PortfOptim_Goals},
  posted-at            = {2017-10-29 14:30:55},
  timestamp            = {2020-02-28 03:37},
}

@Article{Zanella-2015,
  author               = {Zanella, Nicola},
  date                 = {2015-10},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Risk Capacity over the Life Cycle: The Role of Human Capital, High Priority Goals, and Discretionary Wealth},
  doi                  = {10.3905/jwm.2015.18.3.027},
  issn                 = {1534-7524},
  number               = {3},
  pages                = {27--36},
  volume               = {18},
  abstract             = {From the perspective of wealth management, life cycle theoretical models suggest that the total portfolio, including human capital, should play an influential role in determining the composition of financial assets. In fact, by incorporating human capital into portfolio choice optimization, investors become wealthier, holding a respectable amount of safe assets. Young investors should be invested in stocks, and the risky asset share is expected to decline as individuals convert their human wealth into financial capital.

Unfortunately, these academic recommendations and popular wisdom are inconsistent with the empirical observations on portfolio choice at an international level. In this article, stylized facts on income and saving patterns over the life cycle are presented, providing evidence that human wealth should be hump-shaped; younger workers, who do not have substantial wealth, have to implement a hierarchy of saving goals.

Discretionary wealth to invest aggressively in the stock markets is present only during the years of middle-aged prosperity. The glide path used by the actual target-date funds in retirement plans needs to be reconsidered.},
  citeulike-article-id = {13968093},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2015.18.3.027},
  groups               = {Goals based investing, Goals_Invest, Investor_RiskCapacity, Human_Capital, PortfOptim_Goals, Invest_Capacity, Goals_Priority},
  owner                = {cristi},
  posted-at            = {2016-03-05 17:48:23},
  timestamp            = {2020-02-28 03:37},
}

@InBook{Ritter-2019,
  author         = {Ritter, Gordon},
  booktitle      = {Big data and machine learning in quantitative investment},
  date           = {2019},
  title          = {Reinforcement learning in finance},
  doi            = {10.1002/9781119522225.ch12},
  isbn           = {9781119522195},
  location       = {Chichester, UK},
  pages          = {225--250},
  publisher      = {John Wiley \& Sons, Ltd},
  abstract       = {Sutton and Barto said that the key idea of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. A value function is a mathematical expectation in a certain probability space. The underlying probability measure is the one associated to a system which is very familiar to classically trained statisticians: a Markov process. The multivariate Cauchy distribution is elliptical, but its moments of orders 1 and higher are all infinite/undefined. Therefore, it is not mean-variance equivalent because the requisite means and variances would be undefined. In finance, as in certain other fields, the problem of reward function is also subtle, but happily this subtle problem has been solved for us by Bernoulli, Von Neumann and Morgenstern, Arrow and Pratt. Formulating an intelligent behaviour as a reinforcement learning problem begins with identification of the state space and the action space.},
  day            = {30},
  f1000-projects = {QuantInvest},
  groups         = {ML_ReinfoLrng},
  timestamp      = {2020-02-28 03:38},
}

@Misc{Schulman-2016,
  author         = {Schulman, John},
  date           = {2016},
  title          = {Optimizing Expectations: From Deep Reinforcement Learning to Stochastic Computation Graphs},
  type           = {THESIS.DEGREE},
  url            = {https://www2.eecs.berkeley.edu/Pubs/{TechRpts}/2016/{EECS}-2016-217.html},
  abstract       = {This thesis is mostly focused on reinforcement learning, which is viewed as an optimization problem: maximize the expected total reward with respect to the parameters of the policy. The first part of the thesis is concerned with making policy gradient methods more sample-efficient and reliable, especially when used with expressive nonlinear function approximators such as neural networks. Chapter 3 considers how to ensure that policy updates lead to monotonic improvement, and how to optimally update a policy given a batch of sampled trajectories. After providing a theoretical analysis, we propose a practical method called trust region policy optimization (TRPO), which performs well on two challenging tasks: simulated robotic locomotion, and playing Atari games using screen images as input. Chapter 4 looks at improving sample complexity of policy gradient methods in a way that is complementary to TRPO: reducing the variance of policy gradient estimates using a state-value function. Using this method, we obtain state-of-the-art results for learning locomotion controllers for simulated 3D robots.Reinforcement learning can be viewed as a special case of optimizing an expectation, and similar optimization problems arise in other areas of machine learning; for example, in variational inference, and when using architectures that include mechanisms for memory and attention. Chapter 5 provides a unifying view of these problems, with a general calculus for obtaining gradient estimators of objectives that involve a mixture of sampled random variables and differentiable operations. This unifying view motivates applying algorithms from reinforcement learning to other prediction and probabilistic modeling problems.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ReinfoLrng, ML_NumOptimiz},
  institution    = {University of California Berkeley},
  timestamp      = {2020-02-28 03:38},
}

@Article{Stooke-Abbeel-2018,
  author         = {Stooke, Adam and Abbeel, Pieter},
  date           = {2018-03-07},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Accelerated Methods for Deep Reinforcement Learning},
  url            = {https://arxiv.org/abs/1803.02811},
  abstract       = {Deep reinforcement learning (RL) has achieved many recent successes, yet experiment turn-around time remains a key bottleneck in research and in practice. We investigate how to optimize existing deep RL algorithms for modern computers, specifically for a combination of CPUs and GPUs. We confirm that both policy gradient and Q-value learning algorithms can be adapted to learn using many parallel simulator instances. We further find it possible to train using batch sizes considerably larger than are standard, without negatively affecting sample complexity or final performance. We leverage these facts to build a unified framework for parallelization that dramatically hastens experiments in both classes of algorithm. All neural network computations use GPUs, accelerating both data collection and training. Our results include using an entire NVIDIA DGX-1 to learn successful strategies in Atari games in single-digit minutes, using both synchronous and asynchronous algorithms.},
  day            = {7},
  f1000-projects = {QuantInvest},
  groups         = {ML_ReinfoLrng},
  timestamp      = {2020-02-28 03:38},
}

@Article{Wang-Leung-2018,
  author         = {Wang, Ziwei and Leung, Nelson},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Factor Selection with Deep Reinforcement Learning for Financial Forecasting},
  doi            = {10.2139/ssrn.3128678},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3128678},
  abstract       = {The selection of relevant factor set is essential for obtaining a financial forecasting model with high information coefficient (IC), and is challenging due to the non-trivial interaction between factors and their vast combinatorial search space. We introduce a factor selection algorithm using a value network to evaluate the ICs of factor sets. With the fitness function provided by the network evaluation, a genetic algorithm is employed to select a population of candidate factor sets. The ICs of these sets are computed with historical time series data, and the results are subsequently used to train the value network. Such reinforcement iteration can be employed repeatedly to simultaneously improve both the best IC and value network evaluation accuracy. Our approach is generic to any prediction model involving factor selection. To characterize the performance of our algorithm, we employed a typical robust linear regression model with commonly available factors. Although the algorithm's learning process is much like a black-box, the optimal results it delivered can in fact provide useful insights to quantitative researchers. With standard factors available in the financial industry, we achieved a monthly IC of 0.070 on the global market. In comparison, the forward selection and backward elimination algorithms achieved monthly IC of less than 0.05. With a simple portfolio rebalance strategy (monthly rebalance [long/short] with equal weight of the stocks in the [top/bottom] decile of the predicted return), the investment portfolio resulted from our prediction model achieved an average annualized return of 31.17\% with annualized standard deviation of 18.39\% on the global market for 2004-2016.},
  f1000-projects = {QuantInvest},
  groups         = {Factor_Selection, FrcstQWIM_ML, FrcstQWIM_Equity, ML_ReinfoLrng, ML_ForcstTimeSrs, ML_InvestSelect, DeepLearning_QWIM},
  timestamp      = {2020-02-28 03:38},
}

@Electronic{Sharpe-2007,
  author    = {Sharpe, W. F.},
  date      = {2007},
  title     = {Lockbox Separation},
  url       = {https://web.stanford.edu/~wfsharpe/wp/lockbox.pdf},
  abstract  = {This note develops the concept of lockbox separation for retirement financial strategies in a complete market. I show that in such a setting any strategy can be implemented by dividing initial wealth among a series of lockboxes, each designed to fund spending at a particular date using a predetermined investment strategy for managing the funds until that date. Such an approach allows a retiree to pre-commit to follow spending and investment rules throughout the remainder of his or her life. This may have significant advantages since the decisions can be made prior to future reductions in reasoning ability, cognition, etc.. In effect, the current person can act in loco parentis for a potentially diminished future person.},
  groups    = {Goals_Priority},
  owner     = {zkgst0c},
  timestamp = {2020-02-29 09:57},
}

@Article{Simonian-2015,
  author               = {Simonian, Joseph},
  date                 = {2015-01},
  journaltitle         = {Applied Economics Letters},
  title                = {A satisficing approach to factor portfolio construction},
  doi                  = {10.1080/13504851.2014.931911},
  number               = {2},
  pages                = {148--152},
  volume               = {22},
  abstract             = {Multifactor models and the construction of factor-matching portfolios are by now pervasive in investment management. In textbook discussions, the construction of factor-tracking portfolios is presented as a simple exercise solved using well-known optimization methods such as linear programming. While mathematically correct, the standard approach does not address two practical considerations. First, when building portfolios, investment managers typically consider matching the target loadings of some factors as more important than matching the target loadings of other factors. Second, investment managers generally seek to achieve their investment objectives in the most efficient way possible, by minimizing turnover and/or transaction costs. In short, portfolio construction generally requires prioritizing and reconciling multiple objectives. To address the shortcomings of the standard factor-matching framework, we show how a multi-objective optimization methodology known as ?goal programming? can be effectively employed to build portfolios that successfully balance competing investment aims.},
  citeulike-article-id = {13988657},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/13504851.2014.931911},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/13504851.2014.931911},
  day                  = {22},
  groups               = {MultiFactor_Invest, Goals_Priority},
  owner                = {cristi},
  posted-at            = {2016-03-26 22:07:24},
  publisher            = {Routledge},
  timestamp            = {2020-02-29 09:57},
}

@Article{Pfau-2015,
  author               = {Pfau, Wade D.},
  date                 = {2015-03},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Making Sense Out of Variable Spending Strategies for Retirees},
  url                  = {https://ssrn.com/abstract=2579123},
  abstract             = {Variable spending strategies can be situated on a continuum between two extremes: spending a constant amount from the portfolio each year without regard for the remaining portfolio balance, and spending a fixed percentage of the remaining portfolio balance. Variable spending strategies seek compromise between these extremes by avoiding too many spending cuts while also protecting against the risk that spending must subsequently fall to uncomfortably low levels. Two basic categories for variable spending rules explored include decision rule methods and actuarial methods. Ten strategies will be compared using a consistent set of portfolio return and fee assumptions, and using an XYZ formula to calibrate initial spending: the client willingly accepts an X percent probability that spending falls below a threshold of Y dollars (in inflation-adjusted terms) by year Z of retirement. Presenting the distribution of spending and wealth outcomes for different strategies in which the initial spending rate is calibrated with the XYZ formula will allow for a more meaningful comparison of strategies. The article provides a framework for identifying appropriate spending strategies based on client preferences.},
  citeulike-article-id = {13997330},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2579123},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2579123code388906.pdf?abstractid=2579123 and mirid=1},
  day                  = {17},
  groups               = {Retire_Inflation, Goals_Inflation},
  owner                = {cristi},
  posted-at            = {2016-04-05 04:02:51},
  timestamp            = {2020-02-29 09:58},
}

@Article{Pfau-2015c,
  author               = {Pfau, Wade D.},
  date                 = {2015-01},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Choosing a Retirement Income Strategy: A New Evaluation Framework},
  url                  = {https://ssrn.com/abstract=2544651},
  abstract             = {This article presents the initial stages of a new evaluation framework for choosing among retirement income strategies. The investigation includes eight retirement income strategies: constant inflation-adjusted withdrawal amounts, a constant withdrawal percentage of remaining assets, a withdrawal percentage based on remaining life expectancy, a more aggressive hybrid withdrawal percentage, inflation-adjusted and fixed single premium immediate annuities, a variable annuity with a guaranteed living withdrawal benefit rider, and a strategy which annuitizes the flooring level to meet basic needs and uses the hybrid withdrawal percentage for remaining assets. These eight strategies will be analyzed with six retirement outcome measures over a 30-year retirement period: the average amount whereby spending falls below the minimally acceptable level, the average spending amount, the remaining bequest at the end of the retirement period, the minimum spending amount for any year in the retirement period, a measure of whether spending increases or decreases over time defined as spending in the first year divided by spending in the 30th year, and the value of total spending after accounting for diminishing returns from increased spending for a client with somewhat inflexible spending needs. The model is applied to three client scenarios representing a cross-section of RIIA's client segmentation matrix. It is built using Monte Carlo simulations which reflect current market conditions, so that systematic withdrawals and guaranteed products share compatible underlying assumptions.},
  citeulike-article-id = {13997357},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2544651},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2544651code388906.pdf?abstractid=2544651 and mirid=1},
  day                  = {4},
  groups               = {Goals_Inflation, Annuities},
  owner                = {cristi},
  posted-at            = {2016-04-05 04:13:36},
  timestamp            = {2020-02-29 09:58},
}

@Article{Rundle-2018,
  author         = {Rundle, Jonathan},
  date           = {2018-07-31},
  journaltitle   = {The Journal of Retirement},
  title          = {A Social Security Purchase: Is Delaying Social Security More Effective than Purchasing a Deferred Income Annuity?},
  doi            = {10.3905/jor.2018.6.1.008},
  url            = {https://jor.pm-research.com/content/6/1/8},
  abstract       = {This article analyzes a retirement strategy of delaying Social Security commencement past Social Security retirement age to effectively increase annuity income. Using Monte Carlo simulations for returns on a pre-tax retirement portfolio and simplified tax, marriage, and Social Security benefit assumptions, I compare three retirement strategies: 1) following the 4\% rule with retirement at Social Security normal commencement age (66), 2) delayed commencement to age 70 with forgone Social Security income replaced by portfolio withdrawals, and 3) following the 4\% rule with normal retirement age commencement and purchase of deferred life annuity of equivalent expected actuarial value to the increase in Social Security that would occur if commencement were delayed to age 70. Results for the scenarios analyzed generally support delaying Social Security as a preferred method to annuitize assets over the direct purchase of a deferred annuity, particularly under the married scenario. Compared with Strategy 1, delaying Social Security increases portfolio fail rates by year 30 (age 96) by 10 percentage points (to about 20\%) and results in reduced portfolio values across outcomes. The trade-off is increased inflation-protected income under portfolio failure scenarios by 32\% of the primary benefit.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Retire_Tax, Retire_Inflation, Annuities, Goals_Inflation},
  timestamp      = {2020-02-29 09:58},
}

@Article{Sapra-Pedersen-2017,
  author               = {Sapra, Steve and Pedersen, Niels},
  date                 = {2017-04},
  journaltitle         = {The Journal of Retirement},
  title                = {The Role of Long-Maturity TIPS in Retirement Portfolios},
  doi                  = {10.3905/jor.2017.4.4.095},
  issn                 = {2326-6899},
  number               = {4},
  pages                = {95--106},
  volume               = {4},
  abstract             = {This article argues that long-duration Treasury Inflation-Protected Securities (TIPS) should play an important role in the portfolios of workers who are within 10 to 20 years of retirement. Long TIPS effectively help hedge retirees against fluctuations in the real wealth required to sustain a given level of consumption in their retirement years. As such, long-duration TIPS act as the "risk-free asset"in the context of retirement savings, and, for investors on a reasonable savings path, an allocation to this asset class may increase the likelihood of a successful retirement outcome.},
  citeulike-article-id = {14346573},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jor.2017.4.4.095},
  groups               = {Retire_Inflation, Goals_Inflation},
  posted-at            = {2017-04-28 23:30:34},
  timestamp            = {2020-02-29 09:58},
}

@Article{Schaus-et-al-2014,
  author               = {Schaus, Stacy and Allport, Will and Blesy, Justin},
  date                 = {2014-07},
  journaltitle         = {The Journal of Retirement},
  title                = {DC Plans:International Experience and the Need to Address Retirement Security},
  doi                  = {10.3905/jor.2014.2.1.071},
  issn                 = {2326-6899},
  number               = {1},
  pages                = {71--88},
  volume               = {2},
  abstract             = {Despite having participants with similar retirement-outcome goals, the three countries with the greatest reliance on defined-contribution (DC) plans-Australia, the United States, and the United Kingdom-have adopted dramatically different approaches to plan design and investment management. Typical glide paths in these countries, though, tend to have one thing in common: They may leave investors exposed to excessive risk. The authors contrast the key features of DC plans in these countries and call for a more thorough analysis of the risk of loss, as well as the sources of risk, in investment defaults. Glide-path design should focus on desired outcomes, respect risk-capacity limits, and incorporate risk diversification, inflation hedging, and market-shock protection strategies.},
  citeulike-article-id = {14378948},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jor.2014.2.1.071},
  groups               = {Risk_Capacity, Goals_Inflation},
  posted-at            = {2017-06-18 16:42:14},
  timestamp            = {2020-02-29 09:58},
}

@Article{Parker-2016,
  author               = {Parker, Franklin J.},
  date                 = {2016-07},
  journaltitle         = {The Journal of Wealth Management},
  title                = {The Erosion of Portfolio Loss Tolerance over Time: Defining, Defending, and Discussing},
  doi                  = {10.3905/jwm.2016.19.2.023},
  issn                 = {1534-7524},
  number               = {2},
  pages                = {23--31},
  volume               = {19},
  abstract             = {Using two lenses, we explore the role of time in the erosion of loss tolerance of a goal-based investment portfolio. Dubbing this phenomenon goal-based - theta risk, - we also seek to build out the importance of developing tools and techniques for defending against portfolio losses within a goal-based framework. This article reviews the theory of theta risk, explores two adapted techniques for defending the erosion of loss tolerance, then discusses some implications that are illuminated by these techniques. In conclusion, we find that (1) time erodes the loss tolerance of a goal-based portfolio in a quantifiable way, and (2) ironically, more risk (and thus return) from the outset generates more risk tolerance in the end, all else being equal.},
  citeulike-article-id = {14119434},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2016.19.2.023},
  groups               = {Investor_RiskTolerance, Investor_RiskCapacity},
  owner                = {zkgst0c},
  posted-at            = {2016-08-21 12:09:33},
  timestamp            = {2020-02-29 10:03},
}

@Article{Parker-2016a,
  author               = {Parker, Franklin J.},
  date                 = {2016-07},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Portfolio Selection in a Goal-Based Setting},
  doi                  = {10.3905/jwm.2016.19.2.041},
  issn                 = {1534-7524},
  number               = {2},
  pages                = {41--46},
  volume               = {19},
  abstract             = {Using different portfolio ratios, we illustrate the deficiencies of using only modern portfolio theory (MPT) metrics and assumptions when selecting portfolios in a goal-based setting. Through the lenses of the ratios, we show how MPT can choose the wrong, albeit efficient, portfolio to accomplish a goal. These facts illustrate the need for goal-based practitioners to factor in other variables, such as time to a goal and maximum loss thresholds, when managing a portfolio to a goal-oriented mandate.},
  citeulike-article-id = {14119436},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2016.19.2.041},
  owner                = {zkgst0c},
  posted-at            = {2016-08-21 12:11:40},
  timestamp            = {2020-02-29 10:03},
}

@Article{Parker-2016c,
  author               = {Parker, Franklin J.},
  date                 = {2016-10},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Goal-Based Portfolio Optimization},
  doi                  = {10.3905/jwm.2016.19.3.022},
  issn                 = {1534-7524},
  number               = {3},
  pages                = {22--30},
  volume               = {19},
  abstract             = {The article presents a goal-based portfolio optimization approach that is truly native to the goal-based environment. It begins by redefining risk as the probability of failing to attain a specified goal and redefining reward as the excess wealth over and above what is required by the goal. It then presents an optimization procedure that seeks to minimize goal failure and maximize excess return. In preliminary tests, it finds that this goal-based procedure lowers the probability of failing to achieve a specified goal while delivering higher excess wealth than the procedures currently available.},
  citeulike-article-id = {14173607},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2016.19.3.022},
  groups               = {PortfOptim_Goals},
  owner                = {cristi},
  posted-at            = {2016-10-29 23:29:07},
  timestamp            = {2020-02-29 10:03},
}

@Article{Sato-2019,
  author         = {Sato, Yoshiharu},
  date           = {2019-04-10},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Model-Free Reinforcement Learning for Financial Portfolios: A Brief Survey},
  url            = {https://arxiv.org/abs/1904.04973},
  urldate        = {2019-04-21},
  abstract       = {Financial portfolio management is one of the problems that are most frequently encountered in the investment industry. Nevertheless, it is not widely recognized that both Kelly Criterion and Risk Parity collapse into Mean Variance under some conditions, which implies that a universal solution to the portfolio optimization problem could potentially exist. In fact, the process of sequential computation of optimal component weights that maximize the portfolio's expected return subject to a certain risk budget can be reformulated as a discrete-time Markov Decision Process (MDP) and hence as a stochastic optimal control, where the system being controlled is a portfolio consisting of multiple investment components, and the control is its component weights. Consequently, the problem could be solved using model-free Reinforcement Learning (RL) without knowing specific component dynamics. By examining existing methods of both value-based and policy-based model-free RL for the portfolio optimization problem, we identify some of the key unresolved questions and difficulties facing today's portfolio managers of applying model-free RL to their investment portfolios.},
  day            = {10},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:11},
}

@InProceedings{Hu-Lin-2019,
  author         = {Hu, Yuh-Jong and Lin, Shang-Jen},
  booktitle      = {2019 Amity International Conference on Artificial Intelligence (AICAI)},
  date           = {2019-02-04},
  title          = {Deep reinforcement learning for optimizing finance portfolio management},
  doi            = {10.1109/{AICAI}.2019.8701368},
  isbn           = {978-1-5386-9346-9},
  pages          = {14--20},
  publisher      = {IEEE},
  url            = {https://ieeexplore.ieee.org/document/8701368/},
  urldate        = {2019-10-12},
  day            = {4},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:12},
}

@Article{Dong-et-al-2019a,
  author         = {Dong, Linsen and Gao, Guanyu and Li, Yuanlong and Wen, Yonggang},
  date           = {2019-04-23},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Baconian: A Unified Open source Framework for Model-Based Reinforcement Learning},
  url            = {https://arxiv.org/abs/1904.10762},
  urldate        = {2019-05-05},
  abstract       = {Model-Based Reinforcement Learning (MBRL) is one category of Reinforcement Learning (RL) methods which can improve sampling efficiency by modeling and approximating system dynamics. It has been widely adopted in the research of robotics, autonomous driving, etc. Despite its popularity, there still lacks some sophisticated and reusable opensource frameworks to facilitate MBRL research and experiments. To fill this gap, we develop a flexible and modularized framework, Baconian, which allows researchers to easily implement a MBRL testbed by customizing or building upon our provided modules and algorithms. Our framework can free the users from re-implementing popular MBRL algorithms from scratch thus greatly saves the users' efforts.},
  day            = {23},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:13},
}

@Article{FrancoisLavet-et-al-2018,
  author         = {{Francois-Lavet}, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
  date           = {2018},
  journaltitle   = {Foundations and Trends in Machine Learning},
  title          = {An introduction to deep reinforcement learning},
  doi            = {10.1561/2200000071},
  issn           = {1935-8237},
  number         = {3-4},
  pages          = {219--354},
  url            = {http://www.nowpublishers.com/article/Details/{MAL}-071},
  urldate        = {2019-09-15},
  volume         = {11},
  abstract       = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decisionmaking tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:14},
}

@Book{Sutton-Barto-2018,
  author    = {Richard S. Sutton and Andrew G. Barto},
  date      = {2018},
  title     = {Reinforcement Learning: An Introduction},
  edition   = {Second Edition},
  publisher = {MIT Press},
  url       = {http://incompleteideas.net/book/RLbook2018.pdf},
  timestamp = {2020-02-29 10:15},
}

@Book{Beysolow-2019,
  author         = {{Beysolow II}, Taweh},
  date           = {2019},
  title          = {Applied Reinforcement Learning with Python: With OpenAI Gym, Tensorflow, and Keras},
  doi            = {10.1007/978-1-4842-5127-0},
  isbn           = {978-1-4842-5126-3},
  location       = {Berkeley, CA},
  publisher      = {Apress},
  urldate        = {2020-01-03},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:18},
}

@Article{Fujita-et-al-2019,
  author         = {Fujita, Yasuhiro and Kataoka, Toshiki and Nagarajan, Prabhat and Ishikawa, Takahiro},
  date           = {2019-12-09},
  journaltitle   = {arXiv Electronic Journal},
  title          = {ChainerRL: A Deep Reinforcement Learning Library},
  url            = {https://arxiv.org/abs/1912.03905},
  urldate        = {2019-12-13},
  abstract       = {In this paper, we introduce ChainerRL, an open-source Deep Reinforcement Learning (DRL) library built using Python and the Chainer deep learning framework. ChainerRL implements a comprehensive set of DRL algorithms and techniques drawn from the state-of-the-art research in the field. To foster reproducible research, and for instructional purposes, ChainerRL provides scripts that closely replicate the original papers' experimental settings and reproduce published benchmark results for several algorithms. Lastly, ChainerRL offers a visualization tool that enables the qualitative inspection of trained agents. The ChainerRL source code can be found on GitHub: this https URL .},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:40},
}

@Article{Gauci-et-al-2018,
  author         = {Gauci, Jason and Conti, Edoardo and Liang, Yitao and Virochsiri, Kittipat and He, Yuchen and Kaden, Zachary and Narayanan, Vivek and Ye, Xiaohui and Chen, Zhengxing and Fujimoto, Scott},
  date           = {2018-11-01},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Horizon: Facebook's Open Source Applied Reinforcement Learning Platform},
  url            = {https://arxiv.org/abs/1811.00260},
  urldate        = {2019-09-15},
  abstract       = {In this paper we present Horizon, Facebook's open source applied reinforcement learning (RL) platform. Horizon is an end-to-end platform designed to solve industry applied RL problems where datasets are large (millions to billions of observations), the feedback loop is slow (vs. a simulator), and experiments must be done with care because they don't run in a simulator. Unlike other RL platforms, which are often designed for fast prototyping and experimentation, Horizon is designed with production use cases as top of mind. The platform contains workflows to train popular deep RL algorithms and includes data preprocessing, feature transformation, distributed training, counterfactual policy evaluation, optimized serving, and a model-based data understanding tool. We also showcase and describe real examples where reinforcement learning models trained with Horizon significantly outperformed and replaced supervised learning systems at Facebook.},
  day            = {1},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:42},
}

@TechReport{Hofmann-2019,
  author         = {Hofmann, Katja},
  date           = {2019-11-29},
  institution    = {Microsoft},
  title          = {Reinforcement Learning: Past, Present, and Future Perspectives},
  url            = {https://www.microsoft.com/en-us/research/publication/reinforcement-learning-past-present-and-future-perspectives/},
  urldate        = {2019-12-15},
  abstract       = {Reinforcement learning (RL) is a systematic approach to learning and decision making. Developed and studied for decades, recent combinations of RL with modern deep learning have led to impressive demonstrations of the capabilities of today RL systems, and have fueled an explosion of interest and research activity. Join this tutorial to learn about the foundations of RL - elegant ideas that give rise to agents that can learn extremely complex behaviors in a wide range of settings. Broadening out, I give a (subjective) overview of where we currently are in terms of what possible. I conclude with an outlook on key opportunities - both for future research and for real-world applications of RL.},
  day            = {29},
  f1000-projects = {QuantInvest},
  publisher      = {Microsoft},
  timestamp      = {2020-02-29 10:43},
}

@Online{Honchar-2019,
  author    = {Alexandr Honchar},
  date      = {2019},
  title     = {AI for portfolio management: from Markowitz to Reinforcement Learning},
  url       = {https://medium.com/swlh/ai-for-portfolio-management-from-markowitz-to-reinforcement-learning-cffedcbba566},
  abstract  = {The evolution of quantitative asset management techniques with empirical evaluation and Python source code},
  timestamp = {2020-02-29 10:44},
}

@Article{Ivanov-Dyakonov-2019,
  author         = {Ivanov, Sergey and {D'yakonov}, Alexander},
  date           = {2019-06-24},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Modern Deep Reinforcement Learning Algorithms},
  url            = {https://arxiv.org/abs/1906.10025v2},
  urldate        = {2019-10-02},
  abstract       = {Recent advances in Reinforcement Learning, grounded on combining classical theoretical results with Deep Learning paradigm, led to breakthroughs in many artificial intelligence tasks and gave birth to Deep Reinforcement Learning (DRL) as a field of research. In this work latest DRL algorithms are reviewed with a focus on their theoretical justification, practical limitations and observed empirical properties.},
  day            = {24},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:45},
}

@Article{Krishnan-et-al-2016,
  author         = {Krishnan, Sanjay and Garg, Animesh and Liaw, Richard and Miller, Lauren and Pokorny, Florian T. and Goldberg, Ken},
  date           = {2016-04-21},
  journaltitle   = {arXiv Electronic Journal},
  title          = {HIRL: Hierarchical Inverse Reinforcement Learning for Long-Horizon Tasks with Delayed Rewards},
  url            = {https://arxiv.org/abs/1604.06508},
  urldate        = {2019-10-18},
  abstract       = {Reinforcement Learning (RL) struggles in problems with delayed rewards, and one approach is to segment the task into sub-tasks with incremental rewards. We propose a framework called Hierarchical Inverse Reinforcement Learning (HIRL), which is a model for learning sub-task structure from demonstrations. HIRL decomposes the task into sub-tasks based on transitions that are consistent across demonstrations. These transitions are defined as changes in local linearity w.r.t to a kernel function. Then, HIRL uses the inferred structure to learn reward functions local to the sub-tasks but also handle any global dependencies such as sequentiality. We have evaluated HIRL on several standard RL benchmarks: Parallel Parking with noisy dynamics, Two-Link Pendulum, 2D Noisy Motion Planning, and a Pinball environment. In the parallel parking task, we find that rewards constructed with HIRL converge to a policy with an 80\% success rate in 32\% fewer time-steps than those constructed with Maximum Entropy Inverse RL (MaxEnt IRL), and with partial state observation, the policies learned with IRL fail to achieve this accuracy while HIRL still converges. We further find that that the rewards learned with HIRL are robust to environment noise where they can tolerate 1 stdev. of random perturbation in the poses in the environment obstacles while maintaining roughly the same convergence rate. We find that HIRL rewards can converge up-to 6x faster than rewards constructed with IRL.},
  day            = {21},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:47},
}

@Book{Lapan-2018,
  author    = {Maksim Lapan},
  date      = {2018},
  title     = {Deep Reinforcement Learning Hands-On},
  publisher = {Packt},
  url       = {https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands},
  timestamp = {2020-02-29 10:48},
}

@Article{Li-et-al-2019u,
  author         = {Li, Bryan and Cowen-Rivers, Alexander and Kozakowski, Piotr and Tao, David and Kamalakara, Siddhartha and Rajkumar, Nitarshan and Sezhiyan, Hariharan and Huang, Sicong and Gomez, Aidan},
  date           = {2019-10-04},
  journaltitle   = {The Journal of Open Source Software},
  title          = {RL: Generic reinforcement learning codebase in TensorFlow},
  doi            = {10.21105/joss.01524},
  issn           = {2475-9066},
  number         = {42},
  pages          = {1524},
  urldate        = {2020-01-03},
  volume         = {4},
  day            = {4},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:48},
}

@Article{Prollochs-Feuerriegel-2019,
  author         = {Prollochs, Nicolas and Feuerriegel, Stefan},
  date           = {2019-06-12},
  journaltitle   = {The Journal of Open Source Software},
  title          = {ReinforcementLearning: A Package to Perform Model-Free Reinforcement Learning in R},
  doi            = {10.21105/joss.01087},
  issn           = {2475-9066},
  number         = {38},
  pages          = {1087},
  urldate        = {2020-01-03},
  volume         = {4},
  day            = {12},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:49},
}

@Article{Schmidhuber-2019,
  author         = {Schmidhuber, Juergen},
  date           = {2019-12-05},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Reinforcement Learning Upside Down: Don't Predict Rewards -- Just Map Them to Actions},
  url            = {https://arxiv.org/abs/1912.02875v1},
  urldate        = {2019-12-15},
  abstract       = {We transform reinforcement learning (RL) into a form of supervised learning (SL) by turning traditional RL on its head, calling this Upside Down RL (UDRL). Standard RL predicts rewards, while UDRL instead uses rewards as task-defining inputs, together with representations of time horizons and other computable functions of historic and desired future data. UDRL learns to interpret these input observations as commands, mapping them to actions (or action probabilities) through SL on past (possibly accidental) experience. UDRL generalizes to achieve high rewards or other goals, through input commands such as: get lots of reward within at most so much time! A separate paper [61] on first experiments with UDRL shows that even a pilot version of UDRL can outperform traditional baseline algorithms on certain challenging RL problems. We also introduce a related simple but general approach for teaching a robot to imitate humans. First videotape humans imitating the robot's current behaviors, then let the robot learn through SL to map the videos (as input commands) to these behaviors, then let it generalize and imitate videos of humans executing previously unknown behavior. This Imitate-Imitator concept may actually explain why biological evolution has resulted in parents who imitate the babbling of their babies.},
  day            = {5},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:50},
}

@Article{Perrin-Roncalli-2019,
  author         = {Perrin, Sarah and Roncalli, Thierry},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Machine learning optimization algorithms \& portfolio allocation},
  doi            = {10.2139/ssrn.3425827},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3425827},
  urldate        = {2019-08-10},
  abstract       = {Portfolio optimization emerged with the seminal paper of Markowitz (1952). The original mean-variance framework is appealing because it is very efficient from a computational point of view. However, it also has one well-established failing since it can lead to portfolios that are not optimal from a financial point of view (Michaud, 1989). Nevertheless, very few models have succeeded in providing a real alternative solution to the Markowitz model. The main reason lies in the fact that most academic portfolio optimization models are intractable in real life although they present solid theoretical properties. By intractable we mean that they can be implemented for an investment universe with a small number of assets using a lot of computational resources and skills, but they are unable to manage a universe with dozens or hundreds of assets. However, the emergence and the rapid development of robo-advisors means that we need to rethink portfolio optimization and go beyond the traditional mean-variance optimization approach.Another industry and branch of science has faced similar issues concerning large-scale optimization problems. Machine learning and applied statistics have long been associated with linear and logistic regression models. Again, the reason was the inability of optimization algorithms to solve high-dimensional industrial problems. Nevertheless, the end of the 1990s marked an important turning point with the development and the rediscovery of several methods that have since produced impressive results. The goal of this paper is to show how portfolio allocation can benefit from the development of these large-scale optimization algorithms. Not all of these algorithms are useful in our case, but four of them are essential when solving complex portfolio optimization problems. These four algorithms are the coordinate descent, the alternating direction method of multipliers, the proximal gradient method and the Dykstra's algorithm. This paper reviews them and shows how they can be implemented in portfolio allocation.},
  f1000-projects = {QuantInvest},
  groups         = {ML_NumOptimiz},
  timestamp      = {2020-02-29 10:50},
}

@Article{Richard-Roncalli-2019,
  author         = {Richard, Jean-Charles and Roncalli, Thierry},
  date           = {2019-02-15},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Constrained Risk Budgeting Portfolios: Theory, Algorithms, Applications and Puzzles},
  url            = {https://arxiv.org/abs/1902.05710},
  urldate        = {2019-03-07},
  abstract       = {This article develops the theory of risk budgeting portfolios, when we would like to impose weight constraints. It appears that the mathematical problem is more complex than the traditional risk budgeting problem. The formulation of the optimization program is particularly critical in order to determine the right risk budgeting portfolio. We also show that numerical solutions can be found using methods that are used in large-scale machine learning problems. Indeed, we develop an algorithm that mixes the method of cyclical coordinate descent (CCD), alternating direction method of multipliers (ADMM), proximal operators and Dykstra's algorithm. This theoretical body is then applied to some investment problems. In particular, we show how to dynamically control the turnover of a risk parity portfolio and how to build smart beta portfolios based on the ERC approach by improving the liquidity of the portfolio or reducing the small cap bias. Finally, we highlight the importance of the homogeneity property of risk measures and discuss the related scaling puzzle.},
  day            = {15},
  f1000-projects = {QuantInvest},
  groups         = {Risk_Budgeting, Invest_Risk},
  timestamp      = {2020-02-29 10:50},
}

@Conference{Roncalli-2019,
  author    = {Thierry Roncalli},
  booktitle = {SwissQuant Conference},
  date      = {2019},
  title     = {How Machine Learning Can Improve Portfolio Allocation of Robo-Advisors},
  url       = {http://www.thierry-roncalli.com/download/SwissQuant-Conference-Robo-Roncalli-2019.pdf},
  timestamp = {2020-02-29 10:50},
}

@TechReport{Deguest-et-al-2015,
  author               = {Deguest, Romain and Martellini, Lionel and Milhau, Vincent and Suri, Anil and Wang, Hungjen},
  date                 = {2015},
  institution          = {EDHEC Risk Institute},
  title                = {Introducing a Comprehensive Investment Framework for Goals-Based Wealth Management},
  url                  = {https://risk.edhec.edu/publications/introducing-comprehensive-investment-framework-goals-based-wealth-management},
  abstract             = {Any investment process should start with a thorough understanding of the investor problem. Individual investors do not need investment products with alleged superior performance; they need investment solutions that can help them meet their goals subject to prevailing dollar and risk budget constraints.

In this new publication, EDHEC-Risk Institute develops a general operational framework that can be used by financial advisors to allow individual investors to optimally allocate to categories of risks they face across all life stages and wealth segments so as to achieve personally meaningful financial goals.},
  citeulike-article-id = {13911118},
  groups               = {Goals based investing, Individ_Goals},
  howpublished         = {Available at http://faculty-research.edhec.com/research/edhec-publications/2015/introducing-a-comprehensive-risk-allocation-framework-for-goals-based-wealth-management-209580.kjsp},
  owner                = {cristi},
  posted-at            = {2016-01-17 18:08:24},
  timestamp            = {2020-02-29 10:51},
}

@Article{Deguest-et-al-2015a,
  author       = {Deguest, Romain and Martellini, Lionel and Milhau, Vincent},
  date         = {2015},
  journaltitle = {Bankers, Markets and Investors},
  title        = {Mass Customization in Life Cycle Investing Strategies with Income Risk},
  number       = {139},
  pages        = {28--44},
  url          = {https://eska-publishing.com/fr/2014/1124670-mass-customization-in-life-cycle-investing-strategies-with-income-risk-extrait-bmi-139-.html},
  abstract     = {Formal intertemporal portfolio selection models show that the utility maximizing strategy for an individual investor depends on a number of subjective characteristics such as the investor's horizon, risk aversion and non-financial income, in addition to depending on market conditions. These insights are vastly ignored by current forms of target date fund products, which most often propose a deterministic decrease in the equity allocation regardless of market conditions, and ignore the question of labor income risk.

This paper shows that, in spite of a high degree of heterogeneity in individual investors income streams, grouping investors with similar income profiles, and implementing a unique investment strategy for all members of a given class, involves only a limited welfare cost with respect to an idealized fully customized strategy. Our results also suggest that these strategies consistent with mass customization constraints strongly dominate allocation strategies that completely ignore the presence of income risk.},
  groups       = {Human_Capital},
  keywords     = {Human Capital; Income Risk; Optimal Portfolio},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 10:51},
}

@Article{Hayat-et-al-2019,
  author         = {Hayat, Aadil and Singh, Utsav and Namboodiri, Vinay P.},
  date           = {2019-05-24},
  journaltitle   = {arXiv Electronic Journal},
  title          = {InfoRL: Interpretable Reinforcement Learning using Information Maximization},
  url            = {https://arxiv.org/abs/1905.10404},
  abstract       = {Recent advances in reinforcement learning have proved that given an environment we can learn to perform a task in that environment if we have access to some form of a reward function (dense, sparse or derived from IRL). But most of the algorithms focus on learning a single best policy to perform a given set of tasks. In this paper, we focus on an algorithm that learns to not just perform a task but different ways to perform the same task. As we know when the environment is complex enough there always exists multiple ways to perform a task. We show that using the concept of information maximization it is possible to learn latent codes for discovering multiple ways to perform any given task in an environment.},
  day            = {24},
  f1000-projects = {QuantInvest},
  groups         = {ML_Interpretability},
  timestamp      = {2020-02-29 10:51},
}

@Article{Nachum-et-al-2019,
  author         = {Nachum, Ofir and Tang, Haoran and Lu, Xingyu and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  date           = {2019-09-23},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?},
  url            = {https://arxiv.org/abs/1909.10618v1},
  urldate        = {2019-10-02},
  abstract       = {Hierarchical reinforcement learning has demonstrated significant success at solving difficult reinforcement learning (RL) tasks. Previous works have motivated the use of hierarchy by appealing to a number of intuitive benefits, including learning over temporally extended transitions, exploring over temporally extended periods, and training and exploring in a more semantically meaningful action space, among others. However, in fully observed, Markovian settings, it is not immediately clear why hierarchical RL should provide benefits over standard "shallow" RL architectures. In this work, we isolate and evaluate the claimed benefits of hierarchical RL on a suite of tasks encompassing locomotion, navigation, and manipulation. Surprisingly, we find that most of the observed benefits of hierarchy can be attributed to improved exploration, as opposed to easier policy learning or imposed hierarchical structures. Given this insight, we present exploration techniques inspired by hierarchy that achieve performance competitive with hierarchical RL while at the same time being much simpler to use and implement.},
  day            = {23},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:52},
}

@Article{Verma-et-al-2018a,
  author         = {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
  date           = {2018-04-06},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Programmatically Interpretable Reinforcement Learning},
  url            = {https://arxiv.org/abs/1804.02477},
  urldate        = {2019-08-25},
  abstract       = {We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural "oracle". We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.},
  day            = {6},
  f1000-projects = {QuantInvest},
  groups         = {ML_Interpretability, ML_TransferLrng},
  timestamp      = {2020-02-29 10:53},
}

@Article{Zhu-Chen-2019,
  author         = {Zhu, Shengyu and Chen, Zhitang},
  date           = {2019-06-11},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Causal Discovery with Reinforcement Learning},
  url            = {https://arxiv.org/abs/1906.04477},
  urldate        = {2019-09-22},
  abstract       = {Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a directly acyclic graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search (GES), may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use reinforcement learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute corresponding rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real data, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint.},
  day            = {11},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:53},
}

@Article{Chakraborty-2019,
  author         = {Chakraborty, Souradeep},
  date           = {2019-07-09},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Deep Reinforcement Learning in Financial Markets},
  url            = {https://arxiv.org/abs/1907.04373},
  urldate        = {2019-09-07},
  abstract       = {In this paper we explore the usage of deep reinforcement learning algorithms to automatically generate consistently profitable, robust, uncorrelated trading signals in any general financial market. In order to do this, we present a novel Markov decision process (MDP) model to capture the financial trading markets. We review and propose various modifications to existing approaches and explore different techniques to succinctly capture the market dynamics to model the markets. We then go on to use deep reinforcement learning to enable the agent (the algorithm) to learn how to take profitable trades in any market on its own, while suggesting various methodology changes and leveraging the unique representation of the FMDP (financial MDP) to tackle the primary challenges faced in similar works. Through our experimentation results, we go on to show that our model could be easily extended to two very different financial markets and generates a positively robust performance in all conducted experiments.},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:54},
}

@Article{Chakraborty-2019a,
  author         = {Chakraborty, Souradeep},
  date           = {2019-07-09},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Capturing Financial markets to apply Deep Reinforcement Learning},
  url            = {https://arxiv.org/abs/1907.04373v3},
  urldate        = {2019-12-26},
  abstract       = {In this paper we explore the usage of deep reinforcement learning algorithms to automatically generate consistently profitable, robust, uncorrelated trading signals in any general financial market. In order to do this, we present a novel Markov decision process (MDP) model to capture the financial trading markets. We review and propose various modifications to existing approaches and explore different techniques like the usage of technical indicators, to succinctly capture the market dynamics to model the markets. We then go on to use deep reinforcement learning to enable the agent (the algorithm) to learn how to take profitable trades in any market on its own, while suggesting various methodology changes and leveraging the unique representation of the FMDP (financial MDP) to tackle the primary challenges faced in similar works. Through our experimentation results, we go on to show that our model could be easily extended to two very different financial markets and generates a positively robust performance in all conducted experiments.},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:54},
}

@Article{Rafati-Marcia-2018,
  author         = {Rafati, Jacob and Marcia, Roummel F.},
  date           = {2018-11-06},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Deep Reinforcement Learning via L-BFGS Optimization},
  url            = {https://arxiv.org/abs/1811.02693},
  urldate        = {2019-05-05},
  abstract       = {Reinforcement Learning (RL) algorithms allow artificial agents to improve their action selections so as to increase rewarding experiences in their environments. Deep Reinforcement Learning algorithms require solving a nonconvex and nonlinear unconstrained optimization problem. Methods for solving the optimization problems in deep RL are restricted to the class of first-order algorithms, such as stochastic gradient descent (SGD). The major drawback of the SGD methods is that they have the undesirable effect of not escaping saddle points and their performance can be seriously obstructed by ill-conditioning. Furthermore, SGD methods require exhaustive trial and error to fine-tune many learning parameters. Using second derivative information can result in improved convergence properties, but computing the Hessian matrix for large-scale problems is not practical. Quasi-Newton methods require only first-order gradient information, like SGD, but they can construct a low rank approximation of the Hessian matrix and result in superlinear convergence. The limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) approach is one of the most popular quasi-Newton methods that construct positive definite Hessian approximations. In this paper, we introduce an efficient optimization method, based on the limited memory BFGS quasi-Newton method using line search strategy -- as an alternative to SGD methods. Our method bridges the disparity between first order methods and second order methods by continuing to use gradient information to calculate a low-rank Hessian approximations. We provide formal convergence analysis as well as empirical results on a subset of the classic ATARI 2600 games. Our results show a robust convergence with preferred generalization characteristics, as well as fast training time and no need for the experience replaying mechanism.},
  day            = {6},
  f1000-projects = {QuantInvest},
  groups         = {ML_NumOptimiz},
  timestamp      = {2020-02-29 10:54},
}

@Article{Zhang-et-al-2018e,
  author         = {Zhang, Chiyuan and Vinyals, Oriol and Munos, Remi and Bengio, Samy},
  date           = {2018-04-18},
  journaltitle   = {arXiv Electronic Journal},
  title          = {A Study on Overfitting in Deep Reinforcement Learning},
  url            = {https://arxiv.org/abs/1804.06893},
  abstract       = {Recent years have witnessed significant progresses in deep Reinforcement Learning (RL). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging RL problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep RL techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard RL agents and find that they could overfit in various ways. Moreover, overfitting could happen "robustly": commonly used techniques in RL that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in RL. We conclude with a general discussion on overfitting in RL and a study of the generalization behaviors from the perspective of inductive bias.},
  day            = {18},
  f1000-projects = {QuantInvest},
  groups         = {ML_PerfMetrics, ML_Overfitting},
  timestamp      = {2020-02-29 10:55},
}

@InCollection{Abbeel-Ng-2017,
  author         = {Abbeel, Pieter and Ng, Andrew Y.},
  booktitle      = {Encyclopedia of machine learning and data mining},
  date           = {2017},
  title          = {Inverse Reinforcement Learning},
  doi            = {10.1007/978-1-4899-7687-1\_142},
  editor         = {Sammut, Claude and Webb, Geoffrey I.},
  isbn           = {978-1-4899-7685-7},
  location       = {Boston, MA},
  pages          = {678--682},
  publisher      = {Springer US},
  url            = {http://link.springer.com/10.1007/978-1-4899-7687-1\_142},
  urldate        = {2019-10-18},
  abstract       = {Inverse reinforcement learning (inverse RL) considers the problem of extracting a reward function from observed (nearly) optimal behavior of an expert acting in an environment.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:55},
}

@Article{Arora-Doshi-2018,
  author         = {Arora, Saurabh and Doshi, Prashant},
  date           = {2018-06-18},
  journaltitle   = {arXiv Electronic Journal},
  title          = {A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress},
  url            = {https://arxiv.org/abs/1806.06877},
  urldate        = {2019-10-18},
  abstract       = {Inverse reinforcement learning is the problem of inferring the reward function of an observed agent, given its policy or behavior. Researchers perceive IRL both as a problem and as a class of methods. By categorically surveying the current literature in IRL, this article serves as a reference for researchers and practitioners in machine learning to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges which include accurate inference, generalizability, correctness of prior knowledge, and growth in solution complexity with problem size. The article elaborates how the current methods mitigate these challenges. We further discuss the extensions of traditional IRL methods: (i) inaccurate and incomplete perception, (ii) incomplete model, (iii) multiple rewards, and (iv) non-linear reward functions. This discussion concludes with some broad advances in the research area and currently open research questions.},
  day            = {18},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:56},
}

@Article{Brown-Niekum-2019,
  author         = {Brown, Daniel S. and Niekum, Scott},
  date           = {2019-07-17},
  journaltitle   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title          = {Machine teaching for inverse reinforcement learning: algorithms and applications},
  doi            = {10.1609/aaai.v33i01.33017749},
  issn           = {2374-3468},
  pages          = {7749--7758},
  url            = {https://aaai.org/ojs/index.php/{AAAI}/article/view/4771},
  urldate        = {2019-10-18},
  volume         = {33},
  abstract       = {Inverse reinforcement learning (IRL) infers a reward function from demonstrations, allowing for policy improvement and generalization. However, despite much recent interest in IRL, little work has been done to understand the minimum set of demonstrations needed to teach a specific sequential decisionmaking task. We formalize the problem of finding maximally informative demonstrations for IRL as a machine teaching problem where the goal is to find the minimum number of demonstrations needed to specify the reward equivalence class of the demonstrator. We extend previous work on algorithmic teaching for sequential decision-making tasks by showing a reduction to the set cover problem which enables an efficient approximation algorithm for determining the set of maximallyinformative demonstrations. We apply our proposed machine teaching algorithm to two novel applications: providing a lower bound on the number of queries needed to learn a policy using active IRL and developing a novel IRL algorithm that can learn more efficiently from informative demonstrations than a standard IRL approach.},
  day            = {17},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:56},
}

@Article{Buehler-et-al-2019a,
  author         = {Buehler, Hans and Gonon, Lukas and Teichmann, Josef and Wood, Ben and Mohan, Baranidharan and Kochems, Jonathan},
  date           = {2019-03-19},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Deep Hedging: Hedging Derivatives Under Generic Market Frictions Using Reinforcement Learning},
  url            = {https://ssrn.com/abstract=3355706},
  abstract       = {This article discusses a new application of reinforcement learning: to the problem of hedging a portfolio of -the-counter derivatives under under market frictions such as trading costs and liquidity constraints. It is an extended version of our recent work https://www.ssrn.com/abstract=3120710, here using notation more common in the machine learning literature.The objective is to maximize a non-linear risk-adjusted return function by trading in liquid hedging instruments such as equities or listed options. The approach presented here is the first efficient and model-independent algorithm which can be used for such problems at scale.},
  day            = {19},
  f1000-projects = {QuantInvest},
  groups         = {DeepLearning_QWIM},
  timestamp      = {2020-02-29 10:56},
}

@Article{Castro-et-al-2018,
  author         = {Castro, Pablo Samuel and Moitra, Subhodeep and Gelada, Carles and Kumar, Saurabh and Bellemare, Marc G.},
  date           = {2018-12-14},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Dopamine: A Research Framework for Deep Reinforcement Learning},
  url            = {https://arxiv.org/abs/1812.06110},
  urldate        = {2020-01-03},
  abstract       = {Deep reinforcement learning (deep RL) research has grown significantly in recent years. A number of software offerings now exist that provide stable, comprehensive implementations for benchmarking. At the same time, recent deep RL research has become more diverse in its goals. In this paper we introduce Dopamine, a new research framework for deep RL that aims to support some of that diversity. Dopamine is open-source, TensorFlow-based, and provides compact and reliable implementations of some state-of-the-art deep RL agents. We complement this offering with a taxonomy of the different research objectives in deep RL research. While by no means exhaustive, our analysis highlights the heterogeneity of research in the field, and the value of frameworks such as ours.},
  day            = {14},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:57},
}

@Article{Castro-et-al-2019,
  author         = {Castro, Pablo Samuel and Li, Shijian and Zhang, Daqing},
  date           = {2019-07-31},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Inverse Reinforcement Learning with Multiple Ranked Experts},
  url            = {https://arxiv.org/abs/1907.13411},
  urldate        = {2019-10-18},
  abstract       = {We consider the problem of learning to behave optimally in a Markov Decision Process when a reward function is not specified, but instead we have access to a set of demonstrators of varying performance. We assume the demonstrators are classified into one of k ranks, and use ideas from ordinal regression to find a reward function that maximizes the margin between the different ranks. This approach is based on the idea that agents should not only learn how to behave from experts, but also how not to behave from non-experts. We show there are MDPs where important differences in the reward function would be hidden from existing algorithms by the behaviour of the expert. Our method is particularly useful for problems where we have access to a large set of agent behaviours with varying degrees of expertise (such as through GPS or cellphones). We highlight the differences between our approach and existing methods using a simple grid domain and demonstrate its efficacy on determining passenger-finding strategies for taxi drivers, using a large dataset of GPS trajectories.},
  day            = {31},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:57},
}

@Article{Charlier-2019,
  author         = {Charlier, Jeremy},
  date           = {2019-11-23},
  journaltitle   = {arXiv Electronic Journal},
  title          = {From Persistent Homology to Reinforcement Learning with Applications for Retail Banking},
  url            = {https://arxiv.org/abs/1911.11573v1},
  urldate        = {2019-12-15},
  abstract       = {The retail banking services are one of the pillars of the modern economic growth. However, the evolution of the client's habits in modern societies and the recent European regulations promoting more competition mean the retail banks will encounter serious challenges for the next few years, endangering their activities. They now face an impossible compromise: maximizing the satisfaction of their hyper-connected clients while avoiding any risk of default and being regulatory compliant. Therefore, advanced and novel research concepts are a serious game-changer to gain a competitive advantage. In this context, we investigate in this thesis different concepts bridging the gap between persistent homology, neural networks, recommender engines and reinforcement learning with the aim of improving the quality of the retail banking services. Our contribution is threefold. First, we highlight how to overcome insufficient financial data by generating artificial data using generative models and persistent homology. Then, we present how to perform accurate financial recommendations in multi-dimensions. Finally, we underline a reinforcement learning model-free approach to determine the optimal policy of money management based on the aggregated financial transactions of the clients. Our experimental data sets, extracted from well-known institutions where the privacy and the confidentiality of the clients were not put at risk, support our contributions. In this work, we provide the motivations of our retail banking research project, describe the theory employed to improve the financial services quality and evaluate quantitatively and qualitatively our methodologies for each of the proposed research scenarios.},
  day            = {23},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:57},
}

@InProceedings{Harnpadungkij-et-al-2019,
  author         = {Harnpadungkij, Thammasorn and Chaisangmongkon, Warasinee and Phunchongharn, Phond},
  booktitle      = {IEEE 10th International Conference on Awareness Science and Technology (iCAST)},
  date           = {2019-10-23},
  title          = {Risk-Sensitive Portfolio Management by using Distributional Reinforcement Learning},
  doi            = {10.1109/{ICAwST}.2019.8923223},
  isbn           = {978-1-7281-3821-3},
  pages          = {1--6},
  publisher      = {IEEE},
  url            = {https://ieeexplore.ieee.org/document/8923223/},
  urldate        = {2019-12-29},
  day            = {23},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 10:57},
}

@Article{Kolm-Ritter-2019a,
  author         = {Kolm, Petter N. and Ritter, Gordon},
  date           = {2019-09-06},
  journaltitle   = {The Journal of Machine Learning in Finance},
  title          = {Modern Perspectives on Reinforcement Learning in Finance},
  number         = {1},
  url            = {https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=3449401},
  urldate        = {2019-09-07},
  volume         = {1},
  abstract       = {We give an overview and outlook of the field of reinforcement learning as it applies to solving financial applications of intertemporal choice. In finance, common problems of this kind include pricing and hedging of contingent claims, investment and portfolio allocation, buying and selling a portfolio of securities subject to transaction costs, market making, asset liability management and optimization of tax consequences, to name a few. Reinforcement learning allows us to solve these dynamic optimization problems in an almost model-free way, relaxing the assumptions often needed for classical approaches.A main contribution of this article is the elucidation of the link between these dynamic optimization problem and reinforcement learning, concretely addressing how to formulate expected intertemporal utility maximization problems using modern machine learning techniques.},
  day            = {6},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing, ML_InvestSelect},
  timestamp      = {2020-02-29 11:00},
}

@Article{Alsabah-et-al-2020,
  author         = {Alsabah, Humoud and Capponi, Agostino and Ruiz Lacedelli, Octavio and Stern, Matt},
  date           = {2020-01-03},
  journaltitle   = {Journal of Financial Econometrics},
  title          = {Robo-Advising: Learning Investors Risk Preferences via Portfolio Choices*},
  doi            = {10.1093/jjfinec/nbz040},
  issn           = {1479-8409},
  urldate        = {2020-01-17},
  abstract       = {We introduce a reinforcement learning framework for retail robo-advising. The robo-advisor does not know the investor risk preference but learns it over time by observing her portfolio choices in different market environments. We develop an exploration-exploitation algorithm that trades off costly solicitations of portfolio choices by the investor with autonomous trading decisions based on stale estimates of investor risk aversion. We show that the approximate value function constructed by the algorithm converges to the value function of an omniscient robo-advisor over a number of periods that is polynomial in the state and action space. By correcting for the investor mistakes, the robo-advisor may outperform a stand-alone investor, regardless of the investor opportunity cost for making portfolio decisions.},
  day            = {3},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 11:00},
}

@Article{Jomaa-et-al-2019,
  author       = {Hadi S. Jomaa and Josif Grabocka and Lars Schmidt-Thieme},
  date         = {2019},
  journaltitle = {arXiv Electronic Journal},
  title        = {Hyp-RL : Hyperparameter Optimization by Reinforcement Learning},
  url          = {https://arxiv.org/abs/1906.11527},
  abstract     = {Hyperparameter tuning is an omnipresent problem in machine learning as it is an integral aspect of obtaining the state-of-the-art performance for any model. Most often, hyperparameters are optimized just by training a model on a grid of possible hyperparameter values and taking the one that performs best on a validation sample (grid search). More recently, methods have been introduced that build a so-called surrogate model that predicts the validation loss for a specific hyperparameter setting, model and dataset and then sequentially select the next hyperparameter to test, based on a heuristic function of the expected value and the uncertainty of the surrogate model called acquisition function (sequential model-based Bayesian optimization, SMBO).
In this paper we model the hyperparameter optimization problem as a sequential decision problem, which hyperparameter to test next, and address it with reinforcement learning. This way our model does not have to rely on a heuristic acquisition function like SMBO, but can learn which hyperparameters to test next based on the subsequent reduction in validation loss they will eventually lead to, either because they yield good models themselves or because they allow the hyperparameter selection policy to build a better surrogate model that is able to choose better hyperparameters later on. Experiments on a large battery of 50 data sets demonstrate that our method outperforms the state-of-the-art approaches for hyperparameter learning.},
  timestamp    = {2020-02-29 11:01},
}

@Article{Dixon-Halperin-2019,
  author         = {Dixon, Matthew Francis and Halperin, Igor},
  date           = {2019-09-14},
  journaltitle   = {SSRN Electronic Journal},
  title          = {The Four Horsemen of Machine Learning in Finance},
  url            = {https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=3453564},
  urldate        = {2019-09-14},
  abstract       = {Machine Learning has been used in the finance services industry for over 40 years, yet it is only in recent years that it has become more pervasive across investment management and trading. Machine learning provides a more general framework for financial modeling than its linear parametric predecessors, generalizing archetypal modeling approaches, such as factor modeling, derivative pricing, portfolio construction, optimal hedging with model-free, data-driven approaches which are more robust to model risk and capture outliers. Yet despite their demonstrated potential, barriers to adoption have emerged - most of them artifacts of the sociology of this inter-disciplinary field. Based on discussions with several industry experts and the authors' multi-decadal experience using machine learning and traditional quantitative finance at investment banks, asset management and securities trading firms, this position article identifies the major red flags and sets out guidelines and solutions to avoid them. Examples using supervised learning and reinforcement in investment management and trading are provided to illustrate best practices.},
  day            = {14},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-05-25 11:07},
}

@Article{Dixon-Halperin-2020,
  author         = {Dixon, Matthew Francis and Halperin, Igor},
  date           = {2020-02-25},
  journaltitle   = {SSRN Electronic Journal},
  title          = {G-Learner and GIRL:Goal Based Wealth Management with Reinforcement Learning},
  url            = {https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=3543852},
  urldate        = {2020-02-26},
  abstract       = {We present a reinforcement learning approach to goal based wealth management problems such as optimization of retirement plans or target dated funds. In such problems, an investor seeks to achieve a financial goal by making periodic investments in the portfolio while being employed, and periodically draws from the account when in retirement, in addition to the ability to re-balance the portfolio by selling and buying different assets (e.g. stocks). Instead of relying on a utility of consumption, we present G-Learner: a reinforcement learning algorithm that operates with explicitly defined one-step rewards, does not assume a data generation process, and is suitable for noisy data. Our approach is based on G-learning (Fox et al., 2015) - a probabilistic extension of the Q-learning method of reinforcement learning. In this paper, we demonstrate how G-learning, when applied to a quadratic reward and Gaussian reference policy, gives an entropy-regulated Linear Quadratic Regulator (LQR). This critical insight provides a novel and computationally tractable tool for wealth management tasks which scales to high dimensional portfolios. In addition to the solution of the direct problem of G-learning, we also present a new algorithm, GIRL, that extends our goal-based G-learning approach to the setting of Inverse Reinforcement Learning (IRL) where rewards collected by the agent are not observed, and should instead be inferred. We demonstrate that GIRL can successfully learn the reward parameters of a G-Learner agent and thus imitate its behavior. Finally, we discuss potential applications of the G-Learner and GIRL algorithms for wealth management and robo-advising.},
  day            = {25},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-05-25 11:07},
}

@Article{Halperin-2020,
  author         = {Halperin, Igor},
  date           = {2020-04-25},
  journaltitle   = {The Journal of Derivatives},
  title          = {QLBS: Q-Learner in the Black-Scholes(-Merton) Worlds},
  doi            = {10.3905/jod.2020.1.108},
  issn           = {1074-1240},
  pages          = {jod.2020.1.108},
  urldate        = {2020-05-05},
  abstract       = {This article presents a discrete-time option pricing model that is rooted in reinforcement learning (RL), and more specifically in the famous Q-Learning method of RL. We construct a risk-adjusted Markov Decision Process for a discrete-time version of the classical Black-Scholes-Merton (BSM) model, where the option price is an optimal Q-function, while the optimal hedge is a second argument of this optimal Q-function, so that both the price and hedge are parts of the same formula. Pricing is done by learning to dynamically optimize risk-adjusted returns for an option replicating portfolio, as in Markowitz portfolio theory. Using Q-Learning and related methods, once created in a parametric setting, the model can go model-free and learn to price and hedge an option directly from data, without an explicit model of the world. This suggests that RL may provide efficient data-driven and model-free methods for the optimal pricing and hedging of options. Once we depart from the academic continuous-time limit, and vice versa, option pricing methods developed in Mathematical Finance may be viewed as special cases of model-based reinforcement learning. Further, due to the simplicity and tractability of our model, which only needs basic linear algebra (plus Monte Carlo simulation, if we work with synthetic data), and its close relationship to the original BSM model, we suggest that our model could be used in the benchmarking of different RL algorithms for financial trading applications.},
  day            = {25},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-05-25 11:07},
}

@Comment{jabref-meta: databaseType:biblatex;}
